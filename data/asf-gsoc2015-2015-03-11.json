{"ideas": [{"id": "VXQUERY-131", "summary": "Supporting Hadoop data and cluster management", "link": "https://issues.apache.org/jira/browse/VXQUERY-131", "project": "VXQuery", "votes": "0", "watches": "11", "comments": 19, "labels": ["gsoc", "gsoc2015", "hadoop", "java", "mentor", "xml"], "description": "<p>Many organizations support Hadoop. It would be nice to be able to read data from this source. The project will include creating a strategy (with the mentor's guidance) for reading XML data from HDFS and implementing it. When connecting VXQuery to HDFS, the strategy may need to consider how to read sections of an XML file. </p>\n\n<p>In addition, we could use Yarn as our cluster manager. The Apache Hadoop YARN (Yet Another Resource Negotiator) would be a good cluster management tool for VXQuery. If VXQuery can read data from HDFS, then why not also manage the cluster with a tool provided by Hadoop. The solution would replace the current custom python scripts for cluster management.</p>\n\n<p>Goal</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>Read XML from HDFS</li>\n\t<li>Manage the VXQuery cluster with Yarn</li>\n</ul>"}, {"id": "VXQUERY-128", "summary": "XMark Benchmark Support", "link": "https://issues.apache.org/jira/browse/VXQUERY-128", "project": "VXQuery", "votes": "1", "watches": "4", "comments": 4, "labels": ["gsoc", "gsoc2015", "java", "mentor", "xquery"], "description": "<p>Currently only two XMark queries work in VXQuery. The attached issues must be resolved to support all the XMark queries.</p>\n\n<p>The project will involve identifying all the issue related to XMark queries. Each issue will require a plan to fix the issue (To be created in tandem with the mentor). The student will then execute the plan to create error free query execution. As secondary goal, the query should be optimized provide reasonable query times.</p>\n\n<p>Goals</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>All XMark queries execute error free.</li>\n\t<li>The queries are run on a large dataset as a benchmark test.</li>\n\t<li>(stretch goal) The queries are optimized to run efficiently.</li>\n</ul>"}, {"id": "VXQUERY-32", "summary": "VXQuery integration with Apache Lucene", "link": "https://issues.apache.org/jira/browse/VXQUERY-32", "project": "VXQuery", "votes": "1", "watches": "4", "comments": 3, "labels": ["gsoc", "gsoc2015", "java", "mentor", "xml", "xquery"], "description": "<p>XML documents contain textual content and the need to search large corpora of documents is a fairly common task. The intent of this project is to leverage Apache Lucene's indexing and search capability so that users of the VXQuery engine can express and run text-search queries.</p>\n\n<p>This project will have two parts.<br/>\n1. Design and implement the ability for users to create and manage text indices on collections of XML documents.<br/>\n2. Implement functions in VXQuery to exploit these text-indices to execute relevant queries efficiently.</p>\n\n<p>As a starting point, the system does not need to do automatic index selection (decide to use the text index when the query did not actually refer to an index). Instead the functions would be used directly in the query and the system would have to use the said indices.</p>"}, {"id": "TIKA-1540", "summary": "New Tika plugin for image based feature extraction using computer vision techniques", "link": "https://issues.apache.org/jira/browse/TIKA-1540", "project": "Tika", "votes": "1", "watches": "6", "comments": 5, "labels": ["gsoc2015"], "description": "<p>This will be a web-service client based parser to perform image feature extraction using Computer Vision techniques. </p>"}, {"id": "TIKA-1456", "summary": "Visual Sentiment API parser", "link": "https://issues.apache.org/jira/browse/TIKA-1456", "project": "Tika", "votes": "0", "watches": "2", "comments": 2, "labels": ["gsoc2015"], "description": "<p>Integrate the Visual Sentibank API as a parser for images. We can use Aperture from CMU, it's released under the MIT license:</p>\n\n<p><a href=\"https://github.com/d8w/aperture\" class=\"external-link\" rel=\"nofollow\">https://github.com/d8w/aperture</a></p>"}, {"id": "THRIFT-2989", "summary": "Complete CMake build for Apache Thrift", "link": "https://issues.apache.org/jira/browse/THRIFT-2989", "project": "Thrift", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc", "gsoc2015", "travis"], "description": "<p>CMake is a strong fit for the complexity of the Apache Thrift project with all its target languages and platforms.<br/>\nCMake and CPack are already running for C++, C and Java but the support for remaining language libraries is still to come.<br/>\nThe goal of this ticket is to hold all further patches and sub-tasks aiming for a full integration and replacement of the current automake build by the 1.0 release.</p>\n\n<p>The current state can be followed on our continuous integration at <a href=\"https://travis-ci.org/apache/thrift\" class=\"external-link\" rel=\"nofollow\">https://travis-ci.org/apache/thrift</a></p>"}, {"id": "TEZ-2105", "summary": "Totally Sorted Edge with auto-parallelism", "link": "https://issues.apache.org/jira/browse/TEZ-2105", "project": "Apache Tez", "votes": "1", "watches": "7", "comments": 5, "labels": ["gsoc", "gsoc2015", "hadoop", "java", "pig", "tez"], "description": "<p>Pig-on-Tez supports an edge configuration using a sampled Output along with a vertex manager  for automatic parallelism estimation.</p>\n\n<p>This is referred to in the Pig-on-Tez Hadoop Summit presentation.</p>\n\n<p><a href=\"http://www.slideshare.net/Hadoop_Summit/pig-on-tez-low-latency-etl-with-big-data/19\" class=\"external-link\" rel=\"nofollow\">http://www.slideshare.net/Hadoop_Summit/pig-on-tez-low-latency-etl-with-big-data/19</a></p>\n\n<p>Migrating that plan-model into Tez as a native edge type would allow for much more efficient scheduling of the downstream edges and effectively turn the auto-parallelism implementation into a runtime skew-correcting mechanism within this edge.</p>\n\n<p>The Tez Edge has enough information to sample, determine partitioning order and correct parallelism.</p>"}, {"id": "TEZ-2104", "summary": "A CrossProductEdge which produces synthetic cross-product parallelism", "link": "https://issues.apache.org/jira/browse/TEZ-2104", "project": "Apache Tez", "votes": "0", "watches": "3", "comments": 2, "labels": ["gsoc", "gsoc2015", "hadoop", "hive", "java", "tez"], "description": "<p>Instead of producing duplicate data for the synthetic cross-product, to fit into partitions, the amount of net IO can be vastly reduced by a special purpose cross-product data movement edge.</p>\n\n<p>The Shuffle edge routes each partition's output to a single reducer, while the cross-product edge routes it into a matrix of reducers without actually duplicating the disk data.</p>\n\n<p>A partitioning scheme with 3 partitions on the lhs and rhs of a join operation can be routed into 9 reducers by performing a cross-product similar to </p>\n\n<p>(1,2,3) x (a,b,c) = <span class=\"error\">&#91;(1,a), (1,b), (1,c), (2,a), (2,b) ...&#93;</span></p>\n\n<p>This turns a single task cross-product model into a distributed cross product.</p>"}, {"id": "TEZ-2103", "summary": "Implement a Partial completion VertexManagerPlugin", "link": "https://issues.apache.org/jira/browse/TEZ-2103", "project": "Apache Tez", "votes": "0", "watches": "3", "comments": 2, "labels": ["gsoc", "gsoc2015", "hadoop", "java", "tez"], "description": "<p>Currently, there is no sibling communication between tasks - this implies that a task can be completed by the first vertex in a wave of tasks, but the entire wave of tasks has to complete before success can be reported.</p>\n\n<p>This occurs in limit + filter query patterns common between the data access engines.</p>\n\n<div class=\"code panel\" style=\"border-width: 1px;\"><div class=\"codeContent panelContent\">\n<pre class=\"code-java\">\nselect * from data where x &gt; 1 limit 10;\n</pre>\n</div></div>\n\n<p>will run through a full-table scan worth of tasks to generate 10 rows per task, to aggregate it to produce the final 10 row result.</p>\n\n<p>The VertexManager receives counters/events early enough to short-circuit the rest of the vertex tasks, to prevent the remainder of tasks from getting scheduled when the limit condition has been satisfied by an initial sub-set of the tasks.</p>\n\n<p>This is a specialization of the VertexManagerPlugin for this common case scheduling pattern.</p>"}, {"id": "TEZ-145", "summary": "Support a combiner processor that can run non-local to map/reduce nodes", "link": "https://issues.apache.org/jira/browse/TEZ-145", "project": "Apache Tez", "votes": "0", "watches": "5", "comments": 15, "labels": ["gsoc", "gsoc2015", "hadoop", "java", "tez"], "description": "<p>For aggregate operators that can benefit by running in multi-level trees, support of being able to run a combiner in a non-local mode would allow performance efficiencies to be gained by running a combiner at a rack-level. </p>"}, {"id": "STRATOS-1244", "summary": "Showing health statistics in GUI", "link": "https://issues.apache.org/jira/browse/STRATOS-1244", "project": "Stratos", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015", "mentor"], "description": "<p>I remember we had an idea on $subject sometime back. For example, we can show the memory consumption and load average for each member node and LB stats in each cluster.</p>\n\n<p>So we might need to expose those details though rest API, which we can easily get by subscribing the health stat topic.</p>"}, {"id": "STRATOS-1234", "summary": "Software Update Management Solution for Stratos", "link": "https://issues.apache.org/jira/browse/STRATOS-1234", "project": "Stratos", "votes": "1", "watches": "3", "comments": 13, "labels": ["gsoc2015", "mentor"], "description": "<p>Stratos uses Virtual Machines and Containers for hosting platform services on different Infrastructure as a Service (IaaS) solutions. At present Puppet is used for orchestration management on Virtual Machine based systems and manages all required software in Puppet Master. Container based systems creates Docker images for each platform service by including required software in the Docker image itself.</p>\n\n<p>In Virtual Machine use-case VM instances will communicate with Puppet master and execute the software installation. The same approach can be used for applying software updates. </p>\n\n<p>In Docker use-case we do not use Puppet because a new container with required software can be started in few seconds. This is very efficient compared to using Puppet and installing software on demand.</p>\n\n<p>The requirement of this project is to implement a core Stratos feature to propagate software updates in a live PaaS environment.</p>\n\n<p>1. Puppet based solution:</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>Push software updates of a cartridge to Puppet Master (might not need to automate).</li>\n\t<li>Invoke the software update process via the Stratos API for a given application.</li>\n\t<li>Stratos Manager could send a new event to trigger puppet agent in each instance to apply the updates.</li>\n</ul>\n\n\n<p>2. Docker based solution</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>Create a new docker image (with a new image id) for the cartridge with software updates (might not need to automate).</li>\n\t<li>Invoke the software update process via the Stratos API for a given application.</li>\n\t<li>Autoscaler can implement a new feature to bring down existing instances and create new instances with the new docker image id.</li>\n</ul>\n\n\n<p>Important!</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>In each scenario if updates are backward compatible, software update process should execute in phases, it should not bring down the entire cluster to apply the updates. If so the service will be unavailable for a certain time period. The idea is to apply the updates to set of members at a time.</li>\n\t<li>If the updates are not backward compatible, we could make the entire cluster unavailable at once and apply the updates.</li>\n\t<li>Member's state needs to be changed to a new state called \"Updating\" when applying the updates.</li>\n</ul>\n\n\n<p>If there is an interest on doing this project please send a mail to imesh at apache dot org by copying Apache Dev mailing list <span class=\"error\">&#91;1&#93;</span>. Please refer Stratos Wiki <span class=\"error\">&#91;2&#93;</span> for more information on Stratos architecture and how it works.</p>\n\n<p><span class=\"error\">&#91;1&#93;</span> <a href=\"http://stratos.apache.org/community/mailing-lists.html\" class=\"external-link\" rel=\"nofollow\">http://stratos.apache.org/community/mailing-lists.html</a><br/>\n<span class=\"error\">&#91;2&#93;</span> <a href=\"https://cwiki.apache.org/confluence/display/STRATOS\" class=\"external-link\" rel=\"nofollow\">https://cwiki.apache.org/confluence/display/STRATOS</a></p>"}, {"id": "STRATOS-1211", "summary": "Introducing \"curve fitting\" for stat prediction algorithm of Autoscaler", "link": "https://issues.apache.org/jira/browse/STRATOS-1211", "project": "Stratos", "votes": "0", "watches": "3", "comments": 9, "labels": ["gsoc2015"], "description": "<p>This is a summery of a mail sent to Stratos dev under \"<span class=\"error\">&#91;Autoscaling&#93;</span> <span class=\"error\">&#91;Improvement&#93;</span> Introducing \"curve fitting\" for stat prediction algorithm of Autoscaler\" subject.</p>\n\n<p>Current implementation</p>\n\n<p>Currently CEP calculates average, gradient, and second derivative and send those values to Autoscaler. Then Autoscaler predicts the values using S = u*t + 0.5*a*t*t.</p>\n\n<p>In this method CEP calculation is not very much accurate as it does not consider all the events when calculating the gradient and second derivative. Therefore the equation we apply doesn't yield the best prediction.</p>\n\n<p>Proposed Implementation</p>\n\n<p>CEP's task</p>\n\n<p>I think best approach is to do \"curve fitting\"<span class=\"error\">&#91;1&#93;</span> for received event sample in a particular time window. Refer \"Locally weighted linear regression\" section at <span class=\"error\">&#91;2&#93;</span> for more details.</p>\n\n<p>We would need a second degree polynomial fitter for this, where we can use Apache commons math library for this. Refer the sample at <span class=\"error\">&#91;3&#93;</span>, we can run this with any degree. e.g. 2, 3. Just increase the degree to increase the accuracy.</p>\n\n<p>E.g.<br/>\nSo if get degree 2 polynomial fitter, we will have an equation like below where value(v) is our statistic value and time(t) is the time of event.</p>\n\n<p>Equation we get from received events,<br/>\nv = a*t*t + b*t + c</p>\n\n<p>So the solution is,<br/>\nFind memberwise curves that fits events received in specific window(say 10 minutes) at CEP<br/>\nSend the parameters of fitted line(a, b, and c in above equation) with the timestamp of last event(T) in the window, to Autoscaler<br/>\nAutoscaler's task<br/>\nAutoscaler use v = a*t*t + b*t + c function to predict the value in any timestamp from the last timestamp<br/>\nE.g. Say we need to find the value(v) after 1 minute(assuming we carried all the calculations in milliseconds),</p>\n\n<p>v = a * (T+60000) * (T+60000) + b * (T+60000) + c<br/>\nSo we have memberwise predictions and we can find clusterwise prediction by averaging all the memberwise values.</p>"}, {"id": "STRATOS-1187", "summary": "Python based Command Line Tool (CLI) for Stratos", "link": "https://issues.apache.org/jira/browse/STRATOS-1187", "project": "Stratos", "votes": "1", "watches": "6", "comments": 4, "labels": ["gsoc2015", "mentor"], "description": "<p>Currently Stratos has a CLI <span class=\"error\">&#91;2&#93;</span> written in Java. This proposal is to implement a similar CLI in Python to make it much more light-weight. The CLI needs to implement commands for all the REST API methods available in 4.1.0 release. Please refer REST API reference <span class=\"error\">&#91;1&#93;</span> and CLI Reference Guide <span class=\"error\">&#91;2&#93;</span> in the Wiki.</p>\n\n<p>If there is an interest on doing this project please send a mail to imesh at apache dot org by copying Apache Stratos Dev mailing list <span class=\"error\">&#91;3&#93;</span>.</p>\n\n<p><span class=\"error\">&#91;1&#93;</span> <a href=\"https://cwiki.apache.org/confluence/display/STRATOS/4.1.0+Stratos+API+Reference\" class=\"external-link\" rel=\"nofollow\">https://cwiki.apache.org/confluence/display/STRATOS/4.1.0+Stratos+API+Reference</a><br/>\n<span class=\"error\">&#91;2&#93;</span> <a href=\"https://cwiki.apache.org/confluence/display/STRATOS/4.1.0+CLI+Guide\" class=\"external-link\" rel=\"nofollow\">https://cwiki.apache.org/confluence/display/STRATOS/4.1.0+CLI+Guide</a><br/>\n<span class=\"error\">&#91;3&#93;</span> <a href=\"http://stratos.apache.org/community/mailing-lists.html\" class=\"external-link\" rel=\"nofollow\">http://stratos.apache.org/community/mailing-lists.html</a></p>"}, {"id": "STRATOS-500", "summary": "AWS Load Balancing Support for Stratos", "link": "https://issues.apache.org/jira/browse/STRATOS-500", "project": "Stratos", "votes": "0", "watches": "2", "comments": 0, "labels": ["gsoc2015", "mentor"], "description": "<p>Apache Stratos provides a load balancer extension API which could be implemented for supporting third party load balancers. Via this extension API a load balancer could fetch the topology in real time and update its configuration accordingly. A reference implementation of this extension has been done for HAProxy <span class=\"error\">&#91;1&#93;</span>.</p>\n\n<p>The idea of the project is to implement a similar extension to support AWS Load Balancing <span class=\"error\">&#91;2&#93;</span> service via its API <span class=\"error\">&#91;3&#93;</span>. </p>\n\n<p>If there is an interest on doing this project please send a mail to imesh at apache dot org by copying Apache Stratos Dev mailing list <span class=\"error\">&#91;4&#93;</span>.</p>\n\n<p><span class=\"error\">&#91;1&#93;</span> <a href=\"https://github.com/apache/stratos/tree/master/extensions/load-balancer/haproxy-extension\" class=\"external-link\" rel=\"nofollow\">https://github.com/apache/stratos/tree/master/extensions/load-balancer/haproxy-extension</a><br/>\n<span class=\"error\">&#91;2&#93;</span> <a href=\"http://aws.amazon.com/elasticloadbalancing/\" class=\"external-link\" rel=\"nofollow\">http://aws.amazon.com/elasticloadbalancing/</a><br/>\n<span class=\"error\">&#91;3&#93;</span> <a href=\"http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/Welcome.html\" class=\"external-link\" rel=\"nofollow\">http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/Welcome.html</a><br/>\n<span class=\"error\">&#91;4&#93;</span> <a href=\"http://stratos.apache.org/community/mailing-lists.html\" class=\"external-link\" rel=\"nofollow\">http://stratos.apache.org/community/mailing-lists.html</a></p>"}, {"id": "STRATOS-499", "summary": "Google Compute Engine Load Balancing Support for Stratos", "link": "https://issues.apache.org/jira/browse/STRATOS-499", "project": "Stratos", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015", "mentor"], "description": "<p>Apache Stratos provides a load balancer extension API which could be implemented for supporting third party load balancers. Via this extension API a load balancer could fetch the topology in real time and update its configuration accordingly. A reference implementation of this extension has been done for HAProxy <span class=\"error\">&#91;1&#93;</span>.</p>\n\n<p>The idea of the project is to implement a similar extension to support Google Compute Engine Load Balancing service <span class=\"error\">&#91;2&#93;</span> via its CLI or API. </p>\n\n<p>If there is an interest on doing this project please send a mail to imesh at apache dot org by copying Apache Stratos Dev mailing list <span class=\"error\">&#91;3&#93;</span>.</p>\n\n<p><span class=\"error\">&#91;1&#93;</span> <a href=\"https://github.com/apache/stratos/tree/master/extensions/load-balancer/haproxy-extension\" class=\"external-link\" rel=\"nofollow\">https://github.com/apache/stratos/tree/master/extensions/load-balancer/haproxy-extension</a><br/>\n<span class=\"error\">&#91;2&#93;</span> <a href=\"https://developers.google.com/compute/docs/load-balancing/\" class=\"external-link\" rel=\"nofollow\">https://developers.google.com/compute/docs/load-balancing/</a><br/>\n<span class=\"error\">&#91;3&#93;</span> <a href=\"http://stratos.apache.org/community/mailing-lists.html\" class=\"external-link\" rel=\"nofollow\">http://stratos.apache.org/community/mailing-lists.html</a></p>"}, {"id": "STANBOL-1291", "summary": "Phonetic Linking", "link": "https://issues.apache.org/jira/browse/STANBOL-1291", "project": "Stanbol", "votes": "3", "watches": "5", "comments": 2, "labels": ["gsoc2015", "mentoring"], "description": "<p>Add Phonetic based EntityLinking support to Apache Stanbol</p>\n\n<p>The Idea is to</p>\n\n<p>1. start of with a sound file<br/>\n2. use a speech to text engine like <a href=\"https://issues.apache.org/jira/browse/STANBOL-1007\" title=\"Speech to Text Enhancement Engine using CMU Sphinx\" class=\"issue-link\" data-issue-key=\"STANBOL-1007\">STANBOL-1007</a> to get the transcript<br/>\n3. use NLP processing<br/>\n4. use the FST Linking Enigne (<a href=\"https://issues.apache.org/jira/browse/STANBOL-1128\" title=\"Implement a Lucene FST based Entity Linking Engine (based on OpenSextant / SolrTextTagger)\" class=\"issue-link\" data-issue-key=\"STANBOL-1128\"><del>STANBOL-1128</del></a>) to link a SolrIndex configured for Phonetic linking <span class=\"error\">&#91;1&#93;</span>.<br/>\n5. correct the text transcript based on labels of linked entities.</p>\n\n<p>The main question to be answers is if the phonetic matching (step 4) can correctly link Entities even if the writings in the text transcript are incorrect.</p>\n\n<p>Additional things to validate are</p>\n\n<ul>\n\t<li>the quality of the text transcript good enough</li>\n\t<li>does NLP processing still sufficiently well work on text transcripts</li>\n</ul>\n\n\n<p>This will definitely also require adaptations to the FST Linking Engine as the score is currently calculated base on the levenshtein distance of the mention with the best matching label of an entity - what does not make sense for this specific use case. </p>\n\n<p><span class=\"error\">&#91;1&#93;</span> <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PhoneticFilterFactory\" class=\"external-link\" rel=\"nofollow\">http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PhoneticFilterFactory</a></p>"}, {"id": "STANBOL-1156", "summary": "Freebase Entity Disambiguation", "link": "https://issues.apache.org/jira/browse/STANBOL-1156", "project": "Stanbol", "votes": "2", "watches": "3", "comments": 2, "labels": ["disambiguation", "freebase", "gsoc2015", "mentoring"], "description": "<p>Since <a href=\"https://issues.apache.org/jira/browse/STANBOL-1014\" title=\"Create Entityhub Indexing Tool for freebase.com\" class=\"issue-link\" data-issue-key=\"STANBOL-1014\"><del>STANBOL-1014</del></a>, it is possible to generate an EntityHub site for the Freebase Knowledge Base. As part of Google Summer of Code call for 2013, there has been a proposal for Freebase Entity Disambiguation. Proposal details can be found in the following link: <a href=\"http://www.google-melange.com/gsoc/project/google/gsoc2013/adperezmorales/10001\" class=\"external-link\" rel=\"nofollow\">http://www.google-melange.com/gsoc/project/google/gsoc2013/adperezmorales/10001</a>. The disambiguation process for Freebase should also follow the workflow and architecture stablished at <a href=\"https://issues.apache.org/jira/browse/STANBOL-1037\" title=\"Entity Disambiguation for Stanbol\" class=\"issue-link\" data-issue-key=\"STANBOL-1037\">STANBOL-1037</a>.</p>\n\n<p>The project development has been divided in three global tasks:</p>\n\n<p>1. Integration of resources for local disambiguation. Wikilinks (<a href=\"http://www.iesl.cs.umass.edu/data/wiki-links\" class=\"external-link\" rel=\"nofollow\">http://www.iesl.cs.umass.edu/data/wiki-links</a>) is a dataset that provides URLs of webpages, along with the anchor of the links, and the Wikipedia and Freebase pages they link to. As provided, this dataset can be used to get all the surface strings that refer to a Wikipedia page, but further, it can be used to download the webpages and extract the context around the webpages. This contexts can be used for local disambiguation against Content Items mention contexts.</p>\n\n<p>2. Integration of resources for global disambiguation: Freebase is an enormous graphs of related entities and concepts. The structure of this graph can be used to compute groups of entities that are semantically related in a document. For example, we can use the relationship between Michael Jordan and NBA to disambiguate Michael Jordan in a text. The goal of this task is to store the Freebase graph structure in a Neo4j database and provide an API to use it for disambiguation purposes.</p>\n\n<p>3. Disambiguation algorithm: finally, it is necessary to write an algorithm that take into account the local and global disambiguations score in order to refine the confidence values of the EntityAnnotations in the Enhancement Structure</p>"}, {"id": "STANBOL-1007", "summary": "Speech to Text Enhancement Engine using CMU Sphinx", "link": "https://issues.apache.org/jira/browse/STANBOL-1007", "project": "Stanbol", "votes": "5", "watches": "8", "comments": 12, "labels": ["gsoc2015"], "description": "<p>CMUSphinx is a speaker-independent large vocabulary continuous speech recognizer released under BSD style license (<a href=\"http://cmusphinx.sourceforge.net/wiki/\" class=\"external-link\" rel=\"nofollow\">http://cmusphinx.sourceforge.net/wiki/</a>)</p>\n\n<p>This library could be used to add Speech to Text capabilities to Stanbol. It is important that Enhancement Results keep track of the temporal position of the extracted text within the processed media file.</p>"}, {"id": "SPARK-6192", "summary": "Enhance MLlib's Python API (GSoC 2015)", "link": "https://issues.apache.org/jira/browse/SPARK-6192", "project": "Spark", "votes": "1", "watches": "6", "comments": 4, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>This is an umbrella JIRA for <a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=MechCoder\" class=\"user-hover\" rel=\"MechCoder\">Manoj Kumar</a>'s GSoC 2015 project. The main theme is to enhance MLlib's Python API, to make it on par with the Scala/Java API. The main tasks are:</p>\n\n<p>1. For all models in MLlib, provide save/load method. This also<br/>\nincludes save/load in Scala.<br/>\n2. Python API for evaluation metrics.<br/>\n3. Python API for streaming ML algorithms.<br/>\n4. Python API for distributed linear algebra.<br/>\n5. Simplify MLLibPythonAPI using DataFrames. Currently, we use<br/>\ncustomized serialization, making MLLibPythonAPI hard to maintain. It<br/>\nwould be nice to use the DataFrames for serialization.</p>\n\n<p>I'll link the JIRAs for each of the tasks.</p>\n\n<p>Note that this doesn't mean all these JIRAs are pre-assigned to <a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=MechCoder\" class=\"user-hover\" rel=\"MechCoder\">Manoj Kumar</a>. The TODO list will be dynamic based on the backlog.</p>"}, {"id": "SLING-4382", "summary": "GSoC project: large scale multitenant web applications with Sling", "link": "https://issues.apache.org/jira/browse/SLING-4382", "project": "Sling", "votes": "0", "watches": "2", "comments": 2, "labels": ["gsoc2015"], "description": "<p>This GSoC project is about experimenting with large scale multitenancy features in Apache Sling, based on the ideas at <span class=\"error\">&#91;1&#93;</span> and on our existing multitenant support. </p>\n\n<p>Experimental functionality will have to be implemented, and there's space to experiment with different ways of handling multitenancy, from single JVM multitenant systems to container-based federations and maybe multitenant JVMs if available.</p>\n\n<p>The student will have to quickly grasp the inner workings of the Sling core and will need to be creative in combining the available tools in ways that can add value to Sling as far as large scale deployments are concerned.</p>\n\n<p>The deliverables include demo apps and test systems that demonstrate the multitenancy features on concrete use cases, as well as documentation and test reports that should help the Sling team narrow down the design of those features.</p>\n\n<p><span class=\"error\">&#91;1&#93;</span> <a href=\"https://cwiki.apache.org/confluence/display/SLING/Ideas+for+a+multi-tenant+and+multi-module+content+model\" class=\"external-link\" rel=\"nofollow\">https://cwiki.apache.org/confluence/display/SLING/Ideas+for+a+multi-tenant+and+multi-module+content+model</a></p>"}, {"id": "SLING-1437", "summary": "GSoC project: create more unit and integration tests for Sling and expand test coverage information", "link": "https://issues.apache.org/jira/browse/SLING-1437", "project": "Sling", "votes": "0", "watches": "4", "comments": 10, "labels": ["gsoc", "gsoc2010sling", "gsoc2013", "gsoc2015", "java", "mentor", "testing"], "description": "<p>Sling already has fairly good test coverage, but there's always room for improvement. </p>\n\n<p>The goal of this Google Summer of Code project is to create more unit and integration tests for Sling, as well as measure the test coverage that the combination of unit and integration tests provide.</p>\n\n<p>Interested students are welcome to get in touch with the Sling community via its developers mailing list to discuss the project, see <a href=\"http://sling.apache.org/project-information.html#mailing-lists\" class=\"external-link\" rel=\"nofollow\">http://sling.apache.org/project-information.html#mailing-lists</a></p>\n\n<p>The student will have to get familiar with the Sling codebase, identify areas where tests are missing, write unit and integration tests and submit the results as patches that the Sling committers can verify and hopefully apply.</p>"}, {"id": "SAMZA-587", "summary": "Create SystemConsumer and SystemProducer for ActiveMQ", "link": "https://issues.apache.org/jira/browse/SAMZA-587", "project": "Samza", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc2015", "java"], "description": "<p><a href=\"http://activemq.apache.org/\" class=\"external-link\" rel=\"nofollow\">ActiveMQ</a> is a popular messaging queue. It will be great for Samza to be able to read from and write to it. The basic idea is:<br/>\n1. it will be separated into its own package<br/>\n2. it implements SystemAdmin, SystemConsumer, SystemProducer. ( check the org.apache.samza.system package in <a href=\"http://samza.apache.org/learn/documentation/0.8/api/javadocs/\" class=\"external-link\" rel=\"nofollow\">javadoc</a> )<br/>\n3. it is written in Java. Thought previous implementation is using Scala (<a href=\"https://github.com/apache/samza/tree/master/samza-kafka/src/main/scala/org/apache/samza/system/kafka\" class=\"external-link\" rel=\"nofollow\">Kafka Implementation</a> and <a href=\"https://github.com/apache/samza/tree/master/samza-core/src/main/scala/org/apache/samza/system/filereader\" class=\"external-link\" rel=\"nofollow\">FileSystem Implementation</a>), we are moving from Scala to Java. All scala APIs are Java APIs. <br/>\n4. two systems mentioned in (3) could be used as a reference.</p>\n\n<p>Also see <a href=\"https://issues.apache.org/jira/browse/SAMZA-261?focusedCommentId=14349570&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14349570\" class=\"external-link\" rel=\"nofollow\">comment</a>.</p>"}, {"id": "SAMZA-262", "summary": "Create SystemConsumer and SystemProducer for RabbitMQ", "link": "https://issues.apache.org/jira/browse/SAMZA-262", "project": "Samza", "votes": "0", "watches": "6", "comments": 6, "labels": ["gsoc2015", "java"], "description": "<p><a href=\"https://www.rabbitmq.com/\" class=\"external-link\" rel=\"nofollow\">RabbitMQ</a> is a popular messaging queue. It would be good to be able to read to and write from it.  Per the discussion <a href=\"https://issues.apache.org/jira/browse/SAMZA-236?focusedCommentId=13985982&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13985982\" class=\"external-link\" rel=\"nofollow\">about leveldb</a> this support should be separated into its own package and project (jar) for easy testing and severability.</p>"}, {"id": "SAMZA-261", "summary": "Create SystemConsumer and SystemProducer for Kestrel", "link": "https://issues.apache.org/jira/browse/SAMZA-261", "project": "Samza", "votes": "0", "watches": "6", "comments": 12, "labels": ["gsoc2015", "java"], "description": "<p><a href=\"https://github.com/twitter/kestrel\" class=\"external-link\" rel=\"nofollow\">Kestrel</a> is Twitter's messaging queue system. It would be good to be able to read to and write from it.  Per the discussion <a href=\"https://issues.apache.org/jira/browse/SAMZA-236?focusedCommentId=13985982&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13985982\" class=\"external-link\" rel=\"nofollow\">about leveldb</a> this support should be separated into its own package and project (jar) for easy testing and severability.</p>"}, {"id": "SAMZA-200", "summary": "Explore using MySQL changelog as input stream", "link": "https://issues.apache.org/jira/browse/SAMZA-200", "project": "Samza", "votes": "3", "watches": "6", "comments": 2, "labels": ["gsoc2015", "java", "project"], "description": "<p>Samza is designed with good support for database changelogs, but the current open source release is mostly centered around Kafka. It would be good to have out-of-the-box support for some common databases, such as MySQL, as well.</p>\n\n<p><a href=\"http://www.socc2012.org/s18-das.pdf?attredirects=0\" class=\"external-link\" rel=\"nofollow\">Databus</a> is LinkedIn's change capture tool, but the current open source release focuses mainly on Oracle. There is an open source release of <a href=\"https://github.com/linkedin/databus/wiki/Databus-for-MySQL\" class=\"external-link\" rel=\"nofollow\">Databus for MySQL</a>, but it's a proof-of-concept implementation, not the one used by LinkedIn in production. (The one used by LinkedIn requires a patched version of MySQL.) The open source Databus uses <a href=\"https://code.google.com/p/open-replicator/\" class=\"external-link\" rel=\"nofollow\">Open Replicator</a> to connect to a MySQL server as a slave, and parses the binlog to find any inserts, updates or deletes.</p>\n\n<p>I played around a bit with Open Replicator today, and got it working \u2014 a small Scala program that could get a real-time feed of all changes happening in a MySQL database. However, I have some doubts about the quality of the library (the code is not very good, it has only very cursory tests, the original maintainer hasn't touched it for 18 months, and there are reports of nasty bugs &#8211; eg. blowing up on any negative number). There don't seem to be any better Java binlog parsers out there. But I did skim the source of Open Replicator, and it's not too complicated &#8211; it seems quite feasible to write a MySQL binlog parser ourselves.</p>\n\n<p>This is still very much at exploratory stage, but I think it could be really cool to have database changelog support easily available in Samza.</p>"}, {"id": "SAMZA-6", "summary": "Create a performance test suite", "link": "https://issues.apache.org/jira/browse/SAMZA-6", "project": "Samza", "votes": "0", "watches": "6", "comments": 3, "labels": ["gsoc2015", "java", "project"], "description": "<p>We don't really have an performance metrics at this point. There is a single performance test in the samza-test package, but it would be nice to auto-generate a report that measures things like performance of state restore, max mb/s, max msgs/sec, etc. Kafka's perf suite is a useful guideline:</p>\n\n<p><a href=\"https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing\" class=\"external-link\" rel=\"nofollow\">https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing</a></p>"}, {"id": "PIG-4436", "summary": "TPC-DS queries for Pig", "link": "https://issues.apache.org/jira/browse/PIG-4436", "project": "Pig", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc2015"], "description": "<p>Migrate TPC-DS queries to Pig so we can compare performance with other tool</p>"}, {"id": "PIG-4435", "summary": "TPC-DI queries for Pig", "link": "https://issues.apache.org/jira/browse/PIG-4435", "project": "Pig", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc2015"], "description": "<p>Migrate TPC-DI queries to Pig so we can compare performance with other tool.</p>"}, {"id": "PHOENIX-1687", "summary": "Implement missing math built-in POWER function", "link": "https://issues.apache.org/jira/browse/PHOENIX-1687", "project": "Phoenix", "votes": "0", "watches": "4", "comments": 3, "labels": ["gsoc2015", "java"], "description": "<p>Take a look at the typical math functions that are implemented in relational database systems (<a href=\"http://www.postgresql.org/docs/current/static/functions-math.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/current/static/functions-math.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>For this specific task, it is meant to implement missing funstion POWER for Phoenix in Java.</p>"}, {"id": "PHOENIX-1665", "summary": "Implement missing ARRAY built-in functions", "link": "https://issues.apache.org/jira/browse/PHOENIX-1665", "project": "Phoenix", "votes": "2", "watches": "5", "comments": 8, "labels": ["java", "gsoc2015", "mentor", "sql"], "description": "<p>Take a look at the typical ARRAY built-in functions that are implemented in relational database systems (<a href=\"http://www.postgresql.org/docs/9.2/static/functions-array.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/9.2/static/functions-array.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>Examples of missing functions include UNNEST, ARRAY_APPEND, ARRAY_FILL, ARRAY_PREPEND, etc.</p>"}, {"id": "PHOENIX-1664", "summary": "Implement missing binary string built-in functions", "link": "https://issues.apache.org/jira/browse/PHOENIX-1664", "project": "Phoenix", "votes": "0", "watches": "1", "comments": 0, "labels": ["java", "gsoc2015", "mentor", "sql"], "description": "<p>Take a look at the typical binary string functions and bit string operators that are implemented in relational database systems (<a href=\"http://www.postgresql.org/docs/8.1/static/functions-binarystring.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/8.1/static/functions-binarystring.html</a> and <a href=\"http://www.postgresql.org/docs/8.1/static/functions-bitstring.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/8.1/static/functions-bitstring.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>Examples of missing functions include GET_BYTE, SET_BYTE, GET_BIT, SET_BIT, etc and missing operators include &amp;, |, #, ~, &lt;&lt;, and &gt;&gt;. As a guide, examine how ROUND is implemented in Phoenix as an abstract function with concrete functions per type: long, decimal, and date/time types, as many of the existing built-in functions are already defined but are not applicable for binary types.</p>"}, {"id": "PHOENIX-1663", "summary": "Implement missing string built-in functions", "link": "https://issues.apache.org/jira/browse/PHOENIX-1663", "project": "Phoenix", "votes": "0", "watches": "4", "comments": 8, "labels": ["java", "sql", "gsoc2015", "mentor"], "description": "<p>Take a look at the typical string functions that are implemented in relational database systems (<a href=\"http://www.postgresql.org/docs/8.1/static/functions-string.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/8.1/static/functions-string.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>Examples of missing functions include POSITION, OVERLAY, REPEAT, etc.</p>"}, {"id": "PHOENIX-1662", "summary": "Implement missing date/time built-in functions and operators", "link": "https://issues.apache.org/jira/browse/PHOENIX-1662", "project": "Phoenix", "votes": "1", "watches": "2", "comments": 4, "labels": ["java", "sql", "gsoc2015", "mentor"], "description": "<p>Take a look at the typical date functions that are implemented in relational database systems (<a href=\"http://www.postgresql.org/docs/8.1/static/functions-datetime.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/8.1/static/functions-datetime.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>Examples of missing functions include EXTRACT, AGE, the INTERVAL operator, etc. </p>"}, {"id": "PHOENIX-1661", "summary": "Implement built-in functions for JSON", "link": "https://issues.apache.org/jira/browse/PHOENIX-1661", "project": "Phoenix", "votes": "0", "watches": "5", "comments": 4, "labels": ["json", "java", "sql", "gsoc2015", "mentor"], "description": "<p>Take a look at the JSON built-in functions that are implemented in Postgres (<a href=\"http://www.postgresql.org/docs/9.3/static/functions-json.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/9.3/static/functions-json.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>Examples of functions include ARRAY_TO_JSON, ROW_TO_JSON, TO_JSON, etc. The implementation of these built-in functions will be impacted by how JSON is stored in Phoenix. See <a href=\"https://issues.apache.org/jira/browse/PHOENIX-628\" title=\"Support native JSON data type\" class=\"issue-link\" data-issue-key=\"PHOENIX-628\">PHOENIX-628</a>. An initial implementation could work off of a simple text-based JSON representation and then when a native JSON type is implemented, they could be reworked to be more efficient.</p>"}, {"id": "PHOENIX-1660", "summary": "Implement missing math built-in functions", "link": "https://issues.apache.org/jira/browse/PHOENIX-1660", "project": "Phoenix", "votes": "2", "watches": "7", "comments": 14, "labels": ["java", "sql", "gsoc2015", "mentor"], "description": "<p>Take a look at the typical math functions that are implemented in relational database systems (<a href=\"http://www.postgresql.org/docs/current/static/functions-math.html\" class=\"external-link\" rel=\"nofollow\">http://www.postgresql.org/docs/current/static/functions-math.html</a>) and implement the same for Phoenix in Java following this guide: <a href=\"http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\" class=\"external-link\" rel=\"nofollow\">http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html</a></p>\n\n<p>Examples of missing functions include POWER, LOG, EXP, SQRT, CBRT, etc. As a guide, examine how ROUND is implemented in Phoenix as an abstract function with concrete functions per type: long, decimal, and date/time types.</p>"}, {"id": "PHOENIX-1287", "summary": "Use the joni byte[] regex engine in place of j.u.regex", "link": "https://issues.apache.org/jira/browse/PHOENIX-1287", "project": "Phoenix", "votes": "0", "watches": "3", "comments": 3, "labels": ["gsoc2015"], "description": "<p>See <a href=\"https://issues.apache.org/jira/browse/HBASE-11907\" title=\"Use the joni byte[] regex engine in place of j.u.regex in RegexStringComparator\" class=\"issue-link\" data-issue-key=\"HBASE-11907\"><del>HBASE-11907</del></a>. We'd get a 2x perf benefit plus it's driven off of byte[] instead of strings.Thanks for the pointer, <a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell\" class=\"user-hover\" rel=\"apurtell\">Andrew Purtell</a>.</p>"}, {"id": "PHOENIX-1118", "summary": "Provide a tool for visualizing Phoenix tracing information", "link": "https://issues.apache.org/jira/browse/PHOENIX-1118", "project": "Phoenix", "votes": "0", "watches": "2", "comments": 3, "labels": ["java", "sql", "visualization", "gsoc2015", "mentor"], "description": "<p>Currently there's no means of visualizing the trace information provided by Phoenix. We should provide some simple charting over our metrics tables. Take a look at the following JIRA for sample queries: <a href=\"https://issues.apache.org/jira/browse/PHOENIX-1115?focusedCommentId=14323151&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14323151\" class=\"external-link\" rel=\"nofollow\">https://issues.apache.org/jira/browse/PHOENIX-1115?focusedCommentId=14323151&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14323151</a></p>"}, {"id": "PHOENIX-628", "summary": "Support native JSON data type", "link": "https://issues.apache.org/jira/browse/PHOENIX-628", "project": "Phoenix", "votes": "1", "watches": "6", "comments": 4, "labels": ["json", "java", "sql", "gsoc2015", "mentor"], "description": "<p>MongoDB and PostGres do some interesting things with JSON. We should look at adding similar support. For a detailed description, see JSONB support in Postgres: <br/>\n<a href=\"http://www.craigkerstiens.com/2014/03/24/Postgres-9.4-Looking-up\" class=\"external-link\" rel=\"nofollow\">http://www.craigkerstiens.com/2014/03/24/Postgres-9.4-Looking-up</a><br/>\n<a href=\"http://www.depesz.com/2014/03/25/waiting-for-9-4-introduce-jsonb-a-structured-format-for-storing-json/\" class=\"external-link\" rel=\"nofollow\">http://www.depesz.com/2014/03/25/waiting-for-9-4-introduce-jsonb-a-structured-format-for-storing-json/</a><br/>\n<a href=\"http://michael.otacoo.com/postgresql-2/manipulating-jsonb-data-with-key-unique/\" class=\"external-link\" rel=\"nofollow\">http://michael.otacoo.com/postgresql-2/manipulating-jsonb-data-with-key-unique/</a></p>"}, {"id": "PDFBOX-2530", "summary": "Improve PDFDebugger", "link": "https://issues.apache.org/jira/browse/PDFBOX-2530", "project": "PDFBox", "votes": "0", "watches": "3", "comments": 7, "labels": ["gsoc2015"], "description": "<p>(This is an idea for the <a href=\"https://www.google-melange.com/\" class=\"external-link\" rel=\"nofollow\">Google Summer of Code 2015</a>)</p>\n\n<p>Our command line utility PDFDebugger (part of the command line pdfbox-app get it <a href=\"https://pdfbox.apache.org/downloads.html\" class=\"external-link\" rel=\"nofollow\">here</a>, read description <a href=\"https://pdfbox.apache.org/commandline/\" class=\"external-link\" rel=\"nofollow\">here</a>, see the source code <a href=\"https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&amp;sortby=date\" class=\"external-link\" rel=\"nofollow\">here</a>) needs some improvements:</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>hex view</li>\n\t<li>view of non printable characters</li>\n\t<li>saving streams</li>\n\t<li>binary copy &amp; paste</li>\n\t<li>Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)</li>\n\t<li>Copy the current tree string into the clipboard (useful in discussions about details of a PDF)</li>\n\t<li>(Optional, not sure if easy) Jump to specific place in the tree by entering tree string</li>\n\t<li>ability to search in streams (very useful for content streams and meta data)</li>\n\t<li>show images that are streams</li>\n\t<li>show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves</li>\n\t<li>show PDSeparation color</li>\n\t<li>show PDDeviceN colors</li>\n\t<li>show font encodings and characters</li>\n\t<li>edit attributes</li>\n\t<li>edit streams, while keeping or changing the compression filter</li>\n\t<li>save altered PDF</li>\n\t<li>color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the \"bracketing\" of these operators, i.e. understand where a sequence starts and where it ends. (See \"operator summary\" in the PDF Spec) Other \"important\" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.</li>\n</ul>\n\n\n<p>To see a product with a similar purpose that is better than PDFDebugger, watch <a href=\"https://www.youtube.com/watch?v=g-QcU9B4qMc\" class=\"external-link\" rel=\"nofollow\">this video</a>.</p>\n\n<p>I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see <a href=\"https://issues.apache.org/jira/browse/PDFBOX-2401\" title=\"Image has wrong colors after Merge\" class=\"issue-link\" data-issue-key=\"PDFBOX-2401\"><del>PDFBOX-2401</del></a> and search for \"PDFDebugger\".</p>\n\n<p>Prerequisites:</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>java programming, especially the GUI components</li>\n\t<li>the ability to understand existing source code</li>\n</ul>\n\n\n<p>Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.</p>\n\n<p>Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).</p>\n\n<p>Get introduced: <a href=\"https://pdfbox.apache.org/downloads.html#scm\" class=\"external-link\" rel=\"nofollow\">download the source code with svn</a> and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of <a href=\"https://issues.apache.org/jira/browse/PDFBOX-2401\" title=\"Image has wrong colors after Merge\" class=\"issue-link\" data-issue-key=\"PDFBOX-2401\"><del>PDFBOX-2401</del></a>. Read up something about the structure of PDF on the web or from the <a href=\"https://www.adobe.com/devnet/pdf/pdf_reference.html\" class=\"external-link\" rel=\"nofollow\">PDF Specification</a>.</p>\n\n<p>Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to <a href=\"https://issues.apache.org/jira/browse/PDFBOX-1915\" title=\"Implement shading with Coons and tensor-product patch meshes\" class=\"issue-link\" data-issue-key=\"PDFBOX-1915\"><del>PDFBOX-1915</del></a>.</p>"}, {"id": "OPENNLP-758", "summary": "Unsupervised WSD techniques", "link": "https://issues.apache.org/jira/browse/OPENNLP-758", "project": "OpenNLP", "votes": "0", "watches": "4", "comments": 4, "labels": ["gsoc", "gsoc2015", "java", "nlp", "wsd"], "description": "<p>The objective of Word Sense Disambiguation (WSD) is to determine which sense of a word is meant in a particular context. Therefore, WSD is a classification task, where the classes are the different senses of the ambiguous word.</p>\n\n<p>Different techniques are proposed in the academic literature, which fall mainly into two categories: Supervised and Unsupervised.</p>\n\n<p>For this component, we focus on unsupervised techniques: these methods are based on unlabeled data, and do not exploit any manually tagged data.</p>\n\n<p>The object of this project is to create a WSD solution (for English) that implements some unsupervised techniques. For example:</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>Context Clustering</li>\n\t<li>Word Clustering</li>\n\t<li>Cooccurrence Graphs</li>\n\t<li>Overlap of Sense Definitions</li>\n\t<li>Selectional Preferences</li>\n\t<li>Structural Approaches</li>\n\t<li>Etc.</li>\n</ul>"}, {"id": "OPENNLP-757", "summary": "Supervised WSD techniques", "link": "https://issues.apache.org/jira/browse/OPENNLP-757", "project": "OpenNLP", "votes": "0", "watches": "3", "comments": 4, "labels": ["gsoc", "gsoc2015", "java", "nlp", "wsd"], "description": "<p>The objective of Word Sense Disambiguation (WSD) is to determine which sense of a word is meant in a particular context. Therefore, WSD is a classification task, where the classes are the different senses of the ambiguous word.</p>\n\n<p>Different techniques are proposed in the academic literature, which fall mainly into two categories: Supervised and Unsupervised.</p>\n\n<p>For this component, we focus on supervised techniques: these approaches use machine-learning techniques to learn a classifier from labeled training sets.</p>\n\n<p>The object of this project is to create a WSD solution (for English) that implements some supervised techniques. For example:</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>Decision Lists</li>\n\t<li>Decision Trees</li>\n\t<li>Naive Bayes</li>\n\t<li>Neural Networks</li>\n\t<li>Exemplar-Based or Instance-Based Learning</li>\n\t<li>Support Vector Machines</li>\n\t<li>Ensemble Methods</li>\n\t<li>Semi-supervised Disambiguation</li>\n\t<li>Etc.</li>\n</ul>"}, {"id": "OPENMEETINGS-1178", "summary": "GSOC: WebRTC support for OM", "link": "https://issues.apache.org/jira/browse/OPENMEETINGS-1178", "project": "Openmeetings", "votes": "1", "watches": "3", "comments": 2, "labels": ["webrtc", "gsoc2015", "java", "mentor"], "description": "<p>Currently OM is using Flash to send/receive video we need to move to WebRTC</p>\n\n<p>This will require:<br/>\n1) webm file writer: the writer would be a drop-in replacement for the default flv writer. With the assumption that when the non-webm codecs are recorded that the files would only be playable on red5 or some other player if they implement our media types. </p>\n\n<p>2) webm file reader: for VOD playback</p>\n\n<p>3) webrtc input to flash output: this would accept h264 video and PCM audio only; producing a flash compatible stream without requiring any transcoding.</p>\n\n<p>4) webrtc datachannel</p>\n\n<p>5) make OM to be able to send/receive WebRTC audio/video (most probably using existing wicket library)</p>\n\n<p>Links:<br/>\n<span class=\"error\">&#91;1&#93;</span> WebM container description: <a href=\"http://www.webmproject.org/docs/container/\" class=\"external-link\" rel=\"nofollow\">http://www.webmproject.org/docs/container/</a><br/>\n<span class=\"error\">&#91;2&#93;</span> WebRTC video format: <a href=\"https://tools.ietf.org/html/draft-ietf-rtcweb-video-04\" class=\"external-link\" rel=\"nofollow\">https://tools.ietf.org/html/draft-ietf-rtcweb-video-04</a><br/>\n<span class=\"error\">&#91;3&#93;</span> WebRTC datachannel format: <a href=\"https://tools.ietf.org/html/draft-ietf-rtcweb-data-protocol-09\" class=\"external-link\" rel=\"nofollow\">https://tools.ietf.org/html/draft-ietf-rtcweb-data-protocol-09</a></p>"}, {"id": "OPENMEETINGS-554", "summary": "GSOC: screen sharing issues", "link": "https://issues.apache.org/jira/browse/OPENMEETINGS-554", "project": "Openmeetings", "votes": "0", "watches": "3", "comments": 5, "labels": ["gsoc2015", "java", "mentor"], "description": "<p>Existing function of sreen sharing could be expanded and enhanced. For example, the possibility for the transfer on only one application window, regardless of its size. Or the possibility of shared browsing with a locally installed browser. Moreover, certainly an improvement of the used compression method would be a<br/>\nvery good project topic. </p>\n\n<p>Please see more details here:<br/>\n<a href=\"http://markmail.org/message/g36swvjk6j33dwoi#query:+page:1+mid:2l7cvyrvvd6kfbez+state:results\" class=\"external-link\" rel=\"nofollow\">http://markmail.org/message/g36swvjk6j33dwoi#query:+page:1+mid:2l7cvyrvvd6kfbez+state:results</a></p>"}, {"id": "OPENMEETINGS-553", "summary": "GSOC: Need to have possibility to import/export or sync events from OpenMeetings calendar using ical or caldav protocol.", "link": "https://issues.apache.org/jira/browse/OPENMEETINGS-553", "project": "Openmeetings", "votes": "0", "watches": "3", "comments": 9, "labels": ["gsoc2015", "java", "mentor"], "description": "<p>Need to have export and import mechanisms to ICS format so you can display those meetings and sync to your mobile.</p>\n\n<p>Need CalDav protocol support to have possibility of syncing calendars.</p>\n\n<p>See also:<br/>\n<a href=\"http://www.ietf.org/rfc/rfc2445.txt\" class=\"external-link\" rel=\"nofollow\">http://www.ietf.org/rfc/rfc2445.txt</a><br/>\n<a href=\"http://tools.ietf.org/html/rfc6638\" class=\"external-link\" rel=\"nofollow\">http://tools.ietf.org/html/rfc6638</a></p>"}, {"id": "OPENMEETINGS-550", "summary": "GSOC: Improvements for video conferencing with limited bandwidth.", "link": "https://issues.apache.org/jira/browse/OPENMEETINGS-550", "project": "Openmeetings", "votes": "0", "watches": "3", "comments": 2, "labels": ["gsoc2015", "java", "mentor"], "description": "<p>The main idea of this task is to allow client to choose video quality from his side.<br/>\nI.e. client itself could influence the transferred amount of data to it.</p>\n\n<p>From the Sebastian's e-mail:<br/>\n...<br/>\nSo what could be realized is that every stream that is broadcasted from one user via webcam to Red5/OpenMeetings will be re-transcoded into multiple streams (high, middle, low) bandwidth.</p>\n\n<p>So there might be some limitations to that: - \"high\" quality will never be better then the original material. We can't make a picture better then the original. So all re-transcoding will only make the original to lower quality, never to higher. - Re-transcoding has to happen on the server side (and number of streams are limited, we can't provide a stream on the required bandwidth \"on-demand\" for each user, or only with very big effort) - it will require real-time transcoding on server side which is possible with FFMPEG and some integration into Red5. But we would need a very specialized student that is keen and very motiviated as there is hardly any documentation on that available in the internet. What a project makes a success is if all participant know the potential outcome and the tools and methods that are needed to realize that. I would be happy to put this project on our list but it will be difficult to find somebody with the needed skills. <br/>\n...</p>\n\n<p>Please take a look here for more clarification:<br/>\n<a href=\"http://markmail.org/message/g36swvjk6j33dwoi#query:+page:1+mid:mbeg6lfd3wonm6jl+state:results\" class=\"external-link\" rel=\"nofollow\">http://markmail.org/message/g36swvjk6j33dwoi#query:+page:1+mid:mbeg6lfd3wonm6jl+state:results</a></p>"}, {"id": "OPENMEETINGS-549", "summary": "GSOC: Recurrence events support in OpenMeetings calendar.", "link": "https://issues.apache.org/jira/browse/OPENMEETINGS-549", "project": "Openmeetings", "votes": "0", "watches": "5", "comments": 8, "labels": ["gsoc2015", "java", "mentor"], "description": "<p>It would be great to implement recurrence events support in OpenMeetings calendar.</p>\n\n<p>See RFC2445 for more details:<br/>\n<a href=\"http://www.ietf.org/rfc/rfc2445.txt\" class=\"external-link\" rel=\"nofollow\">http://www.ietf.org/rfc/rfc2445.txt</a></p>"}, {"id": "OODT-808", "summary": "Replace OODT's XMLPRC with Avro's RPC", "link": "https://issues.apache.org/jira/browse/OODT-808", "project": "OODT", "votes": "0", "watches": "2", "comments": 0, "labels": ["gsoc2015"], "description": "<p>OODT relies on a legacy <a href=\"http://en.wikipedia.org/wiki/Remote_procedure_call\" class=\"external-link\" rel=\"nofollow\">Remote Procedure Call</a> implementation, namely <a href=\"https://ws.apache.org/xmlrpc/\" class=\"external-link\" rel=\"nofollow\">Apache XML-RPC</a>. Our version of this library currently sits at 2.0.1, which was released on 28th December 2005.</p>\n\n<p>This issue proposes to replace the convoluted XML-RPC logic with <a href=\"http://avro.apache.org\" class=\"external-link\" rel=\"nofollow\">Apache Avro's</a> <a href=\"http://avro.apache.org/docs/1.7.7/spec.html#Protocol+Declaration\" class=\"external-link\" rel=\"nofollow\">RPC Protocol</a> implementation.</p>\n\n<p>Right now xmlrpc implementation logic exists in the following OODT modules:</p>\n<ul>\n\t<li>catalog</li>\n\t<li>commons</li>\n\t<li>crawler</li>\n\t<li>filemgr</li>\n\t<li>mvn</li>\n\t<li>pcs</li>\n\t<li>protocol/api</li>\n\t<li>pushpull</li>\n\t<li>resource</li>\n\t<li>workflow</li>\n</ul>"}, {"id": "OODT-658", "summary": "Implement Apache Gora as suitable storage abstraction for Filemgr", "link": "https://issues.apache.org/jira/browse/OODT-658", "project": "OODT", "votes": "1", "watches": "3", "comments": 2, "labels": ["gsoc2015"], "description": "<p>As a framework, Apache Gora <span class=\"error\">&#91;0&#93;</span> provides an in-memory data model and persistence for big data. It is my goal to build it in to the filemgr component of OODT in the very near future. I will begin work on this now.</p>\n\n<p>Some history on the mailing lists can be found here</p>\n\n<p><span class=\"error\">&#91;0&#93;</span> <a href=\"http://gora.apache.org\" class=\"external-link\" rel=\"nofollow\">http://gora.apache.org</a><br/>\n<span class=\"error\">&#91;1&#93;</span> <a href=\"http://www.mail-archive.com/dev@oodt.apache.org/msg02500.html\" class=\"external-link\" rel=\"nofollow\">http://www.mail-archive.com/dev@oodt.apache.org/msg02500.html</a></p>"}, {"id": "OLINGO-570", "summary": "Implement OData Json Metadocument Serializer/Parser", "link": "https://issues.apache.org/jira/browse/OLINGO-570", "project": "Olingo", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc2015", "java", "mentor", "xml"], "description": "<p>The Olingo project is currently working on implementing the V4 OData specification published by oasis. The current version of the specification only specifies a metadata document in xml format. This can be problematic on android devices where no native xml parser is available. In a newer version of this specification a new metadata format will be specified in Json.</p>\n\n<p>The goal of this issue would be to implement a serializer which can create a payload based on the new specification. Afterwards the Olingo client library must be able to consume the Json metadata document by implementing a parser.</p>\n\n<p>This issue could be implemented by a Student who takes part in the GSoC. The metadata document is already available as an xml format which could give a student a very good entry point to learn more about OData and the Olingo library. Learning opportunities would be about Json payload creation as well as parsing of Json payloads. This would also include understanding the latest OData V4 specification. If you are interested comment this issue or write a mail to dev@olingo.apache.org .</p>\n\n<p>Sources:<br/>\nOlingo Website: <a href=\"http://olingo.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://olingo.apache.org/</a><br/>\nOData.org: <a href=\"http://www.odata.org/\" class=\"external-link\" rel=\"nofollow\">http://www.odata.org/</a></p>"}, {"id": "OLINGO-569", "summary": "Implement an OData V4 sample service using Olingo", "link": "https://issues.apache.org/jira/browse/OLINGO-569", "project": "Olingo", "votes": "0", "watches": "2", "comments": 2, "labels": ["gsoc2015", "java", "mentor", "odata"], "description": "<p>The Olingo project is currently working on implementing the V4 OData specification published by oasis. This implementation relies on a technical sample which is used for unit and integration testing. Unfortunately this sample can only be understood with extensive knowledge about OData. Thus it can`t be used as a sample for user questions or to document Olingo features.</p>\n\n<p>The task of this issue is to implement a meaningful sample service using the Olingo V4 library. This service should have a very simple Entity Data Model and should be used as a showcase for documentation and user questions.</p>\n\n<p>This issue could be implemented by a Student who takes part in the GSoC. It would give the student a very good overview of the OData protocol while using Olingo to implement this service. The scope of this issue could be adjusted by adding a java client which consumes the implemented service. If you are interested comment this issue or write a mail to dev@olingo.apache.org .</p>\n\n<p>Sources:<br/>\nOlingo Website: <a href=\"http://olingo.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://olingo.apache.org/</a><br/>\nOData.org: <a href=\"http://www.odata.org/\" class=\"external-link\" rel=\"nofollow\">http://www.odata.org/</a></p>"}, {"id": "OLINGO-568", "summary": "OData $search Query Parser for Olingo", "link": "https://issues.apache.org/jira/browse/OLINGO-568", "project": "Olingo", "votes": "0", "watches": "1", "comments": 4, "labels": ["gsoc2015", "java", "mentor", "odata"], "description": "<p>The OData V4 specification describes a System Query Option called $search as follows: \"The $search system query option allows clients to request entities matching a free-text search expression.\"</p>\n\n<p>In scope of this JIRA item we would like to implement a parser which can parse such query strings and provide them to an application in an understandable manner.</p>\n\n<p>This issue could be implemented by a Student who takes part in the GSoC as it is a very isolated topic within the Olingo V4 server library. It would still require a broad knowledge of the OData topic to be an opportunity to learn. The scope for the GSoC does not need to include all subtasks which are currently attached to this issue. If you are interested comment this issue or write a mail to dev@olingo.apache.org </p>\n\n<p>Sources:<br/>\nV4 Specification: <a href=\"http://docs.oasis-open.org/odata/odata/v4.0/errata02/os/complete/part2-url-conventions/odata-v4.0-errata02-os-part2-url-conventions-complete.html#_Toc405999475\" class=\"external-link\" rel=\"nofollow\">http://docs.oasis-open.org/odata/odata/v4.0/errata02/os/complete/part2-url-conventions/odata-v4.0-errata02-os-part2-url-conventions-complete.html#_Toc405999475</a><br/>\nOlingo Website: <a href=\"http://olingo.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://olingo.apache.org/</a><br/>\nOData.org: <a href=\"http://www.odata.org/\" class=\"external-link\" rel=\"nofollow\">http://www.odata.org/</a></p>"}, {"id": "ODE-1029", "summary": "Process Instance Visualization for Monitoring Console", "link": "https://issues.apache.org/jira/browse/ODE-1029", "project": "ODE", "votes": "0", "watches": "3", "comments": 4, "labels": ["angularjs", "coffeescript", "css3", "gsoc2015", "html5", "java", "javascript", "typescript"], "description": "<p>ODE will get a new and shiny web-based management console soon. What still is missing is a graphical monitoring tool, which graphically renders a BPEL process and visually annotates it with markers, showing which activities have been executed already, which are pending, which are DPE'd. Since there is no standardized notation for BPEL, the visualization can be chosen freely. A good starting point is <a href=\"https://github.com/BPELtools/BPELviz\" class=\"external-link\" rel=\"nofollow\">https://github.com/BPELtools/BPELviz</a>, which creates static HTML5/CSS3 documents from BPEL files. This project can be included and extended.</p>\n\n<p>As a bonus, the graphical monitoring could integrate with ODEs debug API (which is not yet exposed as Web(service) API but this is then to be discussed.</p>\n\n<p>GSoC applicants should have strong skills in frontend development (HTML5, CSS3, JavaScript, ideally AngularJS, SVG/Canvas, perhaps autolayouting), some understanding of WS-* and a good idea of how process execution works and which information is needed to make the status of process instances visually understandable.</p>"}, {"id": "ODE-794", "summary": "GSoC: Implement task implementation, data handling and WS-* binding for BPMN 2.0", "link": "https://issues.apache.org/jira/browse/ODE-794", "project": "ODE", "votes": "1", "watches": "2", "comments": 4, "labels": ["gsoc2015", "java", "mentor"], "description": "<p>This project aims at providing task implementation, BPMN 2.0 data handling and the integration in the Web services world (i.e. binding of BPMN interface/operation to WSDL portType/operation). Message events and send/receive/service tasks should be capable to integrate with ODE integration layer.</p>\n\n<p>This project is closely related to <a href=\"https://issues.apache.org/jira/browse/ODE-793\" title=\"GSoC: Implement native BPMN 2.0 navigation based on Jacob\" class=\"issue-link\" data-issue-key=\"ODE-793\">ODE-793</a> and requires strong communication skills.</p>"}, {"id": "ODE-793", "summary": "GSoC: Implement native BPMN 2.0 navigation based on Jacob", "link": "https://issues.apache.org/jira/browse/ODE-793", "project": "ODE", "votes": "2", "watches": "3", "comments": 4, "labels": ["gsoc2015", "java", "mentor"], "description": "<p>Implement native BPMN 2.0 support based on Jacob, ODE's virtual process machine. The goal of this project is to have a first prototype running that is capable to deploy a BPMN 2.0 process model and navigate it properly. Task implementations and data objects are yet out of scope of this project, but all concepts (tasks, events, gateways, sequence flow) should be captured.</p>"}, {"id": "ODE-563", "summary": "Clustering", "link": "https://issues.apache.org/jira/browse/ODE-563", "project": "ODE", "votes": "3", "watches": "5", "comments": 5, "labels": ["gsoc2015", "java"], "description": "<p>We need to deploy Apache ODE in clustered environment to meet requirements for one of our projects. <br/>\nPer FAQ, the clustering support is under development, but it is not listed in the Roadmap. <br/>\nWe need to know if clustering support will be provided in the next release i.e. 1.3. <br/>\nWe will greatly appreciate if you let us know when this support will be added or if there is any workaround.</p>"}, {"id": "ODE-41", "summary": "CXF Implementation of the IAPI", "link": "https://issues.apache.org/jira/browse/ODE-41", "project": "ODE", "votes": "2", "watches": "5", "comments": 3, "labels": ["gsoc2015", "java"], "description": "<p>ODE provides an integration layer (IL) abstraction and currently implements ILs for Axis2 and JBI.</p>\n\n<p>Given the healthy growth and success of CXF, an IL implementation for CXF can provide access to more integration features (Camel, more transports, more formats, etc.)</p>\n\n<p>The CXF IL will further reduce the barrier to entry our users will face in considering ODE for their service orchestration needs.</p>\n\n<p>Potential GSoC applicants should show very good Java skills, good knowledge of WS-*, CXF and WS-BPEL (especially about what \"partner links\" are and why they are needed).</p>\n\n<p>(Tammo is willing to mentor this project)</p>"}, {"id": "NUTCH-1936", "summary": "GSoC 2015 - Move Nutch to Hadoop 2.X", "link": "https://issues.apache.org/jira/browse/NUTCH-1936", "project": "Nutch", "votes": "1", "watches": "3", "comments": 7, "labels": ["gsoc2015"], "description": "<p>The Nutch PMC <a href=\"http://www.mail-archive.com/dev%40nutch.apache.org/msg16250.html\" class=\"external-link\" rel=\"nofollow\">discussed</a> ideas for a good 2015 GSoC project. It appears that porting the (trunk) codebase to <a href=\"http://hadoop.apache.org/docs/stable/\" class=\"external-link\" rel=\"nofollow\">Hadoop 2.X</a> seems to an attractive option and one which would present an excellent learning experience for a summer student.</p>\n\n<p>A more comprehensive description of this issue should be included within either a mentor-defined project description or a successful student application.</p>"}, {"id": "MESOS-1018", "summary": "Implement Libprocess Benchmark Suite", "link": "https://issues.apache.org/jira/browse/MESOS-1018", "project": "Mesos", "votes": "0", "watches": "6", "comments": 4, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>Implement a benchmark suite for libprocess to identify potential performance improvements and test for performance regressions.</p>"}, {"id": "MESOS-703", "summary": "master fails to respect updated FrameworkInfo when the framework scheduler restarts", "link": "https://issues.apache.org/jira/browse/MESOS-703", "project": "Mesos", "votes": "1", "watches": "9", "comments": 6, "labels": ["gsoc", "gsoc2015", "mentor", "twitter"], "description": "<p>When I first ran marathon it was running as a personal user and registered with mesos-master as such due to putting an empty string in the user field. When I restarted marathon as \"nobody\", tasks were still being run as the personal user which didn't exist on the slaves. I know marathon was trying to send a FrameworkInfo with nobody listed as the user because I hard coded it in. The tasks wouldn't run as \"nobody\" until I restarted the mesos-master. Each time I restarted the marathon framework, it reregistered with mesos-master and mesos-master wrote to the logs that it detected a failover because the scheduler went away and then came back.</p>\n\n<p>I understand the scheduler failover, but shouldn't mesos-master respect an updated FrameworkInfo when the scheduler re-registers?</p>"}, {"id": "MARMOTTA-593", "summary": "RDF HDT implementation for Sesame RIO", "link": "https://issues.apache.org/jira/browse/MARMOTTA-593", "project": "Marmotta", "votes": "0", "watches": "3", "comments": 0, "labels": ["gsoc", "gsoc2015", "hdt", "java", "linkeddata", "rdf", "sesame"], "description": "<p><a href=\"http://www.rdfhdt.org\" class=\"external-link\" rel=\"nofollow\">RDF HDT</a> is a compact data structure and binary serialization format for RDF that keeps big datasets compressed to save space while maintaining search and browse operations without prior decompression. This makes it an ideal format for storing and sharing RDF datasets on the Web.</p>\n\n<p>Currently the <a href=\"http://www.rdfhdt.org/manual-of-the-java-hdt-library/\" class=\"external-link\" rel=\"nofollow\">Java Implementation</a> only provides bindings for jena RIOT, with a very restrictive license (GPL).</p>\n\n<p>The idea consist on implementing from scratch the <a href=\"http://rdf4j.org/sesame/2.8/apidocs/org/openrdf/rio/Rio.html\" class=\"external-link\" rel=\"nofollow\">Sesame RIO</a> infrastructure (RDFParser/RDFWriter/RDFHandler) for RDF HDT. </p>\n\n<p>The implementation would require to have good knowledge of Java programming, plus some basic understanding of parsers concepts and the RDF data model.</p>"}, {"id": "MARMOTTA-587", "summary": "KiWi support for Crate DB", "link": "https://issues.apache.org/jira/browse/MARMOTTA-587", "project": "Marmotta", "votes": "0", "watches": "2", "comments": 0, "labels": ["crate", "gsoc", "gsoc2015", "kiwi", "rdf", "sql"], "description": "<p><a href=\"https://crate.io/overview\" class=\"external-link\" rel=\"nofollow\">Crate</a> is new database that promises to scale in a cluster environment.</p>\n\n<p>Since it <a href=\"https://crate.io/docs/stable/sql\" class=\"external-link\" rel=\"nofollow\">uses SQL to query</a> and <a href=\"https://crate.io/docs/projects/crate-jdbc\" class=\"external-link\" rel=\"nofollow\">can be accessed via JDBC</a>, it might be worth to investigate if we can support such database in the KiWi triple store.</p>\n\n<p>Crate is available under a dual licensing model: open source version under Apache License 2.0 and commercial on top of the open source product.</p>"}, {"id": "MARMOTTA-584", "summary": "Add GeoSPARQL support to KiWi triple store", "link": "https://issues.apache.org/jira/browse/MARMOTTA-584", "project": "Marmotta", "votes": "0", "watches": "1", "comments": 0, "labels": ["geo", "geodata", "geosparql", "gsoc2015", "sparql"], "description": "<p><a href=\"http://www.opengeospatial.org/standards/geosparql\" class=\"external-link\" rel=\"nofollow\">GeoSPARQL</a> supports representing and querying geospatial data on the Semantic Web. GeoSPARQL defines a vocabulary for representing geospatial data in RDF, and it defines an extension to the SPARQL query language for processing geospatial data.</p>\n\n<p>Add support for such standard to the <a href=\"http://marmotta.apache.org/kiwi\" class=\"external-link\" rel=\"nofollow\">KiWi triple store</a> would be a very interesting feature.</p>"}, {"id": "MARMOTTA-460", "summary": "Hydra Web APIs", "link": "https://issues.apache.org/jira/browse/MARMOTTA-460", "project": "Marmotta", "votes": "0", "watches": "3", "comments": 4, "labels": ["gsoc", "gsoc2014", "gsoc2015"], "description": "<p>Provide support for Hydra Web APIs:<br/>\n<a href=\"http://hydra-cg.com/\" class=\"external-link\" rel=\"nofollow\">http://hydra-cg.com/</a></p>\n\n<p>I suppose that this can be done within a GSOC project.</p>"}, {"id": "MARMOTTA-435", "summary": "Marmotta CQRS", "link": "https://issues.apache.org/jira/browse/MARMOTTA-435", "project": "Marmotta", "votes": "1", "watches": "2", "comments": 0, "labels": ["cqrs", "gsoc2014", "gsoc2015", "java", "linkeddata", "rest", "rww"], "description": "<p>CQRS (Command Query Responsibility Segregation) is a pattern based on the simple notion that you can use a different model to update information than the model you use to read information. Further details at <a href=\"http://martinfowler.com/bliki/CQRS.html\" class=\"external-link\" rel=\"nofollow\">http://martinfowler.com/bliki/CQRS.html</a></p>\n\n<p>Knowing the issues that some people have integrating Read-Write Linked Data, particularly LDP or SPARQL Update, into the business logic of their application, I do believe that exploring the application of this pattern could help how others interact with Marmotta. And maybe some ideas about RDF patching could be used here: <a href=\"http://www.w3.org/2012/ldp/wiki/LDP_PATCH_Proposals\" class=\"external-link\" rel=\"nofollow\">http://www.w3.org/2012/ldp/wiki/LDP_PATCH_Proposals</a></p>\n\n<p>The implementation would include the server-side (JAX-RS) and a reference implementation of a generic client (preferable in Java, although other languages could be also accepted), and it could has in JSON-LD one of its implementation pillars, but this is just an idea.</p>"}, {"id": "MARMOTTA-432", "summary": "OAuth2 support", "link": "https://issues.apache.org/jira/browse/MARMOTTA-432", "project": "Marmotta", "votes": "1", "watches": "4", "comments": 4, "labels": ["gsoc2014", "gsoc2015", "oauth", "oauth2"], "description": "<p>OAuth2 is a very popular authentication protocol to grant access to third-party applications. Even if the approach has not been explored so far by the Linked Data paradigm, it could be interesting yo offer a OAuth2 provider, including simple admin panel, to see how such</p>\n\n<p>The implemented mechanism should co-exist with the current security module (implemented via HTTP Auth) and covering any of the REST Web Services currently implemented by Marmotta, including Linked Data, SPARQL and the upcoming LDP.</p>\n\n<p>The required technologies would be mainly Java form the server-side, extending the current JAX-RS compliant web service; and some HTML5+JS in the client side.</p>"}, {"id": "MARMOTTA-408", "summary": "Use MBeans / JMX for monitoring  and statistics", "link": "https://issues.apache.org/jira/browse/MARMOTTA-408", "project": "Marmotta", "votes": "0", "watches": "4", "comments": 9, "labels": ["gsoc2014", "gsoc2015", "java", "jmx"], "description": "<p>We should use JMX for monitoring different aspects of the Marmotta system, e.g. the caching statistics. This should replace the existing StatisticsService, providing a new infrastructure to store and push via JMX different parameters, allowing to monitor Marmotta from any of the available JMX tools.</p>"}, {"id": "MARMOTTA-222", "summary": "Kick-off the support to WebID", "link": "https://issues.apache.org/jira/browse/MARMOTTA-222", "project": "Marmotta", "votes": "1", "watches": "2", "comments": 0, "labels": ["gsoc2014", "gsoc2015", "ssl", "tls", "webid"], "description": "<p>Actually Marmotta already offers a kind of support to WebID Identity, but it'd be nice if we could also have support WebID Authentication Protocol over TLS.</p>\n\n<p>Further details at the latest draft of the specification: <a href=\"https://dvcs.w3.org/hg/WebID/raw-file/tip/spec/index-respec.html\" class=\"external-link\" rel=\"nofollow\">https://dvcs.w3.org/hg/WebID/raw-file/tip/spec/index-respec.html</a></p>"}, {"id": "MARMOTTA-202", "summary": "OpenRefine import engine", "link": "https://issues.apache.org/jira/browse/MARMOTTA-202", "project": "Marmotta", "votes": "0", "watches": "3", "comments": 4, "labels": ["gsoc2013", "gsoc2014", "gsoc2015", "refine"], "description": "<p>OpenRefine (formerly Google Refine) is a free power tool for working with messy data. Although it is quite nice as user interface to deal with transforming, reconciling and exporting the data in different formats, it becomes harder when several files with same layout need to transformed in batch mode.</p>\n\n<p>This feature aims to integrate the Refine engine with a completelly different approach we did in LMF (<a href=\"http://code.google.com/p/lmf/wiki/GoogleRefineExtension\" class=\"external-link\" rel=\"nofollow\">http://code.google.com/p/lmf/wiki/GoogleRefineExtension</a> ). The idea, once you refined your data, you can export your project ( <a href=\"http://github.com/OpenRefine/OpenRefine/wiki/Exporters#exporting-projects\" class=\"external-link\" rel=\"nofollow\">http://github.com/OpenRefine/OpenRefine/wiki/Exporters#exporting-projects</a> ); Marmotta should be able to import those files with data compatible by following the script exported from OpenRefine </p>"}, {"id": "KYLIN-591", "summary": "Leverage Zeppelin to interactive with Kylin", "link": "https://issues.apache.org/jira/browse/KYLIN-591", "project": "Kylin", "votes": "1", "watches": "3", "comments": 0, "labels": ["gsoc2015"], "description": "<p>Detail to add...</p>"}, {"id": "JENA-664", "summary": "GeoSPARQL support for Jena", "link": "https://issues.apache.org/jira/browse/JENA-664", "project": "Apache Jena", "votes": "0", "watches": "2", "comments": 4, "labels": ["features", "gsoc2015"], "description": "<p>I am aware that GeoSPARQL support is available in Parliament (<a href=\"http://parliament.semwebcentral.org/\" class=\"external-link\" rel=\"nofollow\">http://parliament.semwebcentral.org/</a>) but that is using the somewhat obsolete Joseki and not Fuseki.</p>"}, {"id": "JENA-647", "summary": "SPARQL template queries", "link": "https://issues.apache.org/jira/browse/JENA-647", "project": "Apache Jena", "votes": "0", "watches": "3", "comments": 0, "labels": ["gsoc", "gsoc2015", "java", "web"], "description": "<p>This enhancement would added predefined query templates to Fuseki.  A query could be executed by calling the URL of the template together with any template parameters.  It would an HTTP URL that could be bookmarked or passed around.  Coupled with the ability to set the result format, it makes getting CSV or JSON </p>\n\n<p>See also a related project in <a href=\"https://issues.apache.org/jira/browse/JENA-632\" title=\"Generate JSON from SPARQL directly.\" class=\"issue-link\" data-issue-key=\"JENA-632\">JENA-632</a>.</p>"}, {"id": "JENA-629", "summary": "Support on-line rebuilds of a TDB store", "link": "https://issues.apache.org/jira/browse/JENA-629", "project": "Apache Jena", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015"], "description": "<p>TDB should occasionally sync its data into a fresh store and then transparently swap over to the new store. This would mean that stores with a lot of churn don't grow to excessive sizes.</p>\n\n<p>\"Occasionally\" could be determined by some (configurable?) heuristic, such as \"every X triples removed\", or when initiated by the user.</p>\n\n<p>My understanding of how TDB works is probably rather sketchy, but I suspect it'd be possible to enumerate the triples in the store and pipe them to tdbloader for a new store. The process could release the read lock periodically so as not to block writes for long periods, but record what happens in those writes for replaying onto the new store at the end. Eventually (and soon, in non-pathological cases) the two stores would be almost identical and TDB could stop writes to the old store, finish replaying any queued writes to the new store, and make the switch.</p>"}, {"id": "JENA-624", "summary": "Develop a new in-memory RDF Dataset implementation", "link": "https://issues.apache.org/jira/browse/JENA-624", "project": "Apache Jena", "votes": "0", "watches": "2", "comments": 0, "labels": ["gsoc", "gsoc2015", "java", "linked_data", "rdf"], "description": "<p>The current (Jan 2014) Jena in-memory dataset uses a general purpose container that works for any storage technology for graphs together with in-memory graphs.  </p>\n\n<p>This project would develop a new implementation design specifically for RDF datasets (triples and quads) and efficient SPARQL execution, for example, using multi-core parallel operations and/or multi-version concurrent datastructures to maximise true parallel operation.</p>\n\n<p>This is a system project suitable for someone interested in datatbase implementation, datastructure design and implementation, operating systems or distributed systems.</p>\n\n<p>Note that TDB can operate in-memory using a simulated disk with copy-in/copy-out semantics for disk-level operations.  It is for faithful testing TDB infrastructure and is not designed performance, general in-memory use or use at scale.  While lesson may be learnt from that system, TDB in-memory is not the answer here.</p>"}, {"id": "JENA-508", "summary": "Add support for XPath 3 Functions", "link": "https://issues.apache.org/jira/browse/JENA-508", "project": "Apache Jena", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc", "gsoc2015"], "description": "<p>XPath 3 is now a Candidate Recommendation - <a href=\"http://www.w3.org/TR/xpath-functions-30/\" class=\"external-link\" rel=\"nofollow\">http://www.w3.org/TR/xpath-functions-30/</a></p>\n\n<p>It contains many new functions and operators (particularly in the mathematical space) which we should consider adding into future versions of ARQ.</p>"}, {"id": "JENA-491", "summary": "Extend CONSTRUCT to build quads", "link": "https://issues.apache.org/jira/browse/JENA-491", "project": "Apache Jena", "votes": "0", "watches": "3", "comments": 3, "labels": ["gsoc", "gsoc2015", "java", "linked_data", "rdf", "sparql"], "description": "<p>This would be an extension to SPARQL.</p>\n\n<p>1/ Add use of GRAPH inside a CONSTRUCT template see SPARQL Update.</p>\n\n<p>2/ Add conneg for quads to Fuseki.</p>\n\n<p>3/ New QueryExecution operations execConstructQuads() and execConstructDataset()</p>\n\n<p>If asked for triples, and the CONSTRUCT generates quads, the named graph items are dropped - that is, only the default graph is returned.  This is for commonality with RIOT.</p>"}, {"id": "JENA-382", "summary": "SPARQL Updates are not cancelable", "link": "https://issues.apache.org/jira/browse/JENA-382", "project": "Apache Jena", "votes": "0", "watches": "4", "comments": 4, "labels": ["cancellation", "gsoc", "gsoc2015", "sparql", "transactions", "update"], "description": "<p>Currently queries are cancelable but updates are not.  Updates are just of capable of running awry especially since some of them can embed queries within them.  In general any update that generates a large amount of data can result in a long running update.</p>\n\n<p>Therefore we should make it possible to cancel updates, my rough proposal for this is as follows:</p>\n\n<p>1 - Provide a cancel() method on UpdateProcessor<br/>\n2 - Provide a cancel() method on individual update commands so a processor can cancel() whichever is currently running</p>\n\n<p>When updates are cancelled it must be treated as if updates failed and the transaction should be rolled back or marked as uncommittable as appropriate.</p>\n\n<p>I will create a SVN branch to experiment on this feature.</p>"}, {"id": "JCLOUDS-829", "summary": "NIO.2 FileSystem facade to BlobStore", "link": "https://issues.apache.org/jira/browse/JCLOUDS-829", "project": "jclouds", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015"], "description": "<p>jclouds could provide a Java 7 NIO.2 <tt>FileSystem</tt> facade to <tt>BlobStore</tt> to enable non-jclouds applications to access blobstores.  Some operations will like writing via an <tt>OutputStream</tt> will require some rethinking since some blobstores, e.g., AWS-S3, require an explicit Content-Length before writing.</p>"}, {"id": "JCLOUDS-826", "summary": "JDBC blobstore", "link": "https://issues.apache.org/jira/browse/JCLOUDS-826", "project": "jclouds", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015"], "description": "<p>jclouds could offer a JDBC blobstore as an alternative to the existing transient and filesystem blobstores.  This could store metadata in something like H2 and the actual blob data on a filesystem, similar to Swift.  This would work around issues like <a href=\"https://bugs.openjdk.java.net/browse/JDK-8030048\" class=\"external-link\" rel=\"nofollow\">https://bugs.openjdk.java.net/browse/JDK-8030048</a> which prevents use of metadata with the filesystem blobstore on Mac OS X.</p>"}, {"id": "JCLOUDS-483", "summary": "Create a proper LoadBalancer abstraction", "link": "https://issues.apache.org/jira/browse/JCLOUDS-483", "project": "jclouds", "votes": "2", "watches": "5", "comments": 2, "labels": ["gsoc2015"], "description": "<p>The current Load Balancer abstraction is in beta, and provides very little functionality. It is hard to create a load balancer that is actually useful if one uses only the portable abstraction.</p>\n\n<p>It would be great to take it to the next level and refactor it and add all missing features. Currently there are only two providers that implement it: Amazon ELB and Rackspace Cloud Load Balancers. Taking the features they expose as a reference to build a proper abstraction can help improving and making really useful so it can be used in the real world.</p>"}, {"id": "JCLOUDS-482", "summary": "Add support for arbitrary CPU and RAM in the ComputeService", "link": "https://issues.apache.org/jira/browse/JCLOUDS-482", "project": "jclouds", "votes": "0", "watches": "2", "comments": 3, "labels": ["gsoc2015"], "description": "<p>Some providers such as Abiquo or CloudSigma do not have the concept of Hardware Profiles and allow users to specify arbitrary CPU and RAM values.</p>\n\n<p>The current ComputeService abstraction assumes all providers have Hardware Profiles, and forces implementations of such providers to provide a  fixed (hardcoded) list just to conform the interface.</p>\n\n<p>It would be great to modernize the Compute workflow so Hardware Profiles are not mandatory and users can manually set their values when needed.</p>"}, {"id": "JCLOUDS-479", "summary": "Web UI interface to jclouds", "link": "https://issues.apache.org/jira/browse/JCLOUDS-479", "project": "jclouds", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc2015"], "description": "<p>It would be really cool to create a Web UI interface to jclouds to allow users to manage credentials, compute instances, and blobstores.  This would be similar to the provider-specific portals that AWS and others offer.  We could implement this a number of different ways, e.g., GAE, Sinatra/JRuby, etc.  Tagging for GSoC 2014.</p>"}, {"id": "HIVE-9689", "summary": "Store distinct value estimator's bit vectors in metastore", "link": "https://issues.apache.org/jira/browse/HIVE-9689", "project": "Hive", "votes": "0", "watches": "3", "comments": 0, "labels": ["gsoc2015"], "description": "<p>Hive currently uses PCSA (Probabilistic Counting and Stochastic Averaging) algorithm to determine distinct cardinality. The NDV value determined from the UDF is stored in the metastore instead of the actual bit vectors. This makes it impossible to estimation the overall NDV across all the partition (or selected partitions). We should ideally store the bitvectors in the metastore and do server side merging of the bitvectors. Also we could replace the current PCSA algorithm in favour of HyperLogLog if space is a constraint. </p>"}, {"id": "HIVE-9688", "summary": "Support SAMPLE operator in hive", "link": "https://issues.apache.org/jira/browse/HIVE-9688", "project": "Hive", "votes": "0", "watches": "3", "comments": 4, "labels": ["gsoc2015"], "description": "<p>Hive needs SAMPLE operator to support parallel order by, skew joins and count + distinct optimizations. Random, Reservoir and Stratified sampling should cover most of the cases.</p>"}, {"id": "HIVE-9687", "summary": "Blink DB style approximate querying in hive", "link": "https://issues.apache.org/jira/browse/HIVE-9687", "project": "Hive", "votes": "0", "watches": "4", "comments": 0, "labels": ["gsoc2015"], "description": "<p><a href=\"http://www.cs.berkeley.edu/~sameerag/blinkdb_eurosys13.pdf\" class=\"external-link\" rel=\"nofollow\">http://www.cs.berkeley.edu/~sameerag/blinkdb_eurosys13.pdf</a></p>\n\n<p>There are various pieces here that need to be thought through and implemented. For e.g. sampling offline, run-time sampling selection module etc.</p>"}, {"id": "HIVE-9523", "summary": "when columns on which tables are partitioned are used in the join condition same join optimizations as for bucketed tables should be applied", "link": "https://issues.apache.org/jira/browse/HIVE-9523", "project": "Hive", "votes": "0", "watches": "3", "comments": 0, "labels": ["gsoc2015"], "description": "<p>For JOIN conditions where partitioning criteria are used respectively:<br/>\n            \u22ee <br/>\nFROM TabA JOIN TabB<br/>\n   ON TabA.partCol1 = TabB.partCol2<br/>\n   AND TabA.partCol2 = TabB.partCol2</p>\n\n<p>the optimizer could/should choose to treat it the same way as with bucketed tables: \u22ee <br/>\nFROM TabC<br/>\n  JOIN TabD<br/>\n     ON TabC.clusteredByCol1 = TabD.clusteredByCol2<br/>\n   AND TabC.clusteredByCol2 = TabD.clusteredByCol2</p>\n\n<p>and use either Bucket Map Join or better, the Sort Merge Bucket Map Join.</p>\n\n<p>This is based on fact that same way as buckets translate to separate files, the partitions essentially provide the same mapping.<br/>\nWhen data locality is known the optimizer could focus only on joining corresponding partitions rather than whole data sets.</p>\n\n<p>#side notes:<br/>\n\u29bf Currently Table DDL Syntax where Partitioning and Bucketing defined at the same time is allowed:<br/>\nCREATE TABLE<br/>\n \u22ee<br/>\nPARTITIONED BY(\u2026) CLUSTERED BY(\u2026) INTO \u2026 BUCKETS;</p>\n\n<p>But in this case optimizer never chooses to use Bucket Map Join or Sort Merge Bucket Map Join which defeats the purpose of creating BUCKETed tables in such scenarios. Should that be raised as a separate BUG?</p>\n\n<p>\u29bf Currently partitioning and bucketing are two separate things but serve same purpose - shouldn't the concept be merged (explicit/implicit partitions?)</p>"}, {"id": "FLINK-1617", "summary": "GSoC project: Query optimisation layer for Flink Streaming", "link": "https://issues.apache.org/jira/browse/FLINK-1617", "project": "Flink", "votes": "0", "watches": "3", "comments": 3, "labels": ["gsoc2015", "java", "streaming"], "description": "<p>Flink streaming currently only supports a limited set of optimisations applied on the streaming programs such as operator chaining, and several optimisations for windowing computations.</p>\n\n<p>The goal of this project would be to extend the  optimiser functionality to include other available query optimisation techniques. This would be a critical feature in the system as it could potentially bring major performance improvements depending on the query.</p>\n\n<p>A comprehensive list of optimisation techniques for streaming queries can be found here:</p>\n\n<p><a href=\"http://hirzels.com/martin/papers/csur14-streamopt.pdf\" class=\"external-link\" rel=\"nofollow\">http://hirzels.com/martin/papers/csur14-streamopt.pdf</a></p>\n\n<p>New ideas are also very welcome, the possibilities are endless. </p>"}, {"id": "FLINK-1541", "summary": "GSoC project: Distributed profiling", "link": "https://issues.apache.org/jira/browse/FLINK-1541", "project": "Flink", "votes": "0", "watches": "1", "comments": 0, "labels": ["distributed", "gsoc2015", "java", "scala"], "description": "<p>Understanding why a given program behaves in a certain way is often difficult and requires in many cases profiling. The profiling allows to detect hot spots in a program which are worthwhile for further investigation. For non-distributed applications there exists a wide variety of useful tools which achieve exactly that. However, Flink currently lacks the capability to profile distributed jobs.</p>\n\n<p>Therefore, we would like to set up the infrastructure necessary for distributed profiling. First, this requires a thorough evaluation of existing tools in order to see how they could be used for profiling within Flink. </p>\n\n<p>The profiling will most likely comprises the following steps:</p>\n\n<p>1. Initiating the system and user code instrumentation<br/>\n2. Collecting the profiling information on the distributed components<br/>\n3. Aggregation of the generated profiling information and a proper visualization</p>"}, {"id": "FLINK-1538", "summary": "GSoC project: Spatial Data Processing Library", "link": "https://issues.apache.org/jira/browse/FLINK-1538", "project": "Flink", "votes": "0", "watches": "2", "comments": 5, "labels": ["gsoc2015", "java", "spatial"], "description": "<p>Processing of geo-spatial data is compute intensive. If done on a large scale as for example when joining millions of GPS traces with huge spatial data sets such as OpenStreetMap, spatial processing can benefit from massively parallel processing platforms such as Apache Flink. </p>\n\n<p>We propose a project to draft and bootstrap a geo-spatial processing library for Apache Flink.</p>\n\n<p>The focus of this project lies on the design of the library. Key features should be a good integration with other Flink APIs, good extensibility, and compatibility with existing data formats. The library could include a variety of processing functionality such as:</p>\n\n<ul>\n\t<li>import of common data formats</li>\n\t<li>fuzzy spatial join for different applications\n\t<ul>\n\t\t<li>containment in area</li>\n\t\t<li>minimum distance to object</li>\n\t\t<li>intersection of polygons</li>\n\t\t<li>...</li>\n\t</ul>\n\t</li>\n\t<li>spatial partitioning techniques</li>\n\t<li>...</li>\n</ul>"}, {"id": "FLINK-1537", "summary": "GSoC project: Machine learning with Apache Flink", "link": "https://issues.apache.org/jira/browse/FLINK-1537", "project": "Flink", "votes": "2", "watches": "5", "comments": 11, "labels": ["gsoc2015", "java", "machine_learning", "scala"], "description": "<p>Currently, the Flink community is setting up the infrastructure for a machine learning library for Flink. The goal is to provide a set of highly optimized ML algorithms and to offer a high level linear algebra abstraction to easily do data pre- and post-processing. By defining a set of commonly used data structures on which the algorithms work it will be possible to define complex processing pipelines. </p>\n\n<p>The Mahout DSL constitutes a good fit to be used as the linear algebra language in Flink. It has to be evaluated which means have to be provided to allow an easy transition between the high level abstraction and the optimized algorithms.</p>\n\n<p>The machine learning library offers multiple starting points for a GSoC project. Amongst others, the following projects are conceivable.</p>\n\n<ul>\n\t<li>Extension of Flink's machine learning library by additional ML algorithms\n\t<ul>\n\t\t<li>Stochastic gradient descent</li>\n\t\t<li>Distributed dual coordinate ascent</li>\n\t\t<li>SVM</li>\n\t\t<li>Gaussian mixture EM</li>\n\t\t<li>DecisionTrees</li>\n\t\t<li>...</li>\n\t</ul>\n\t</li>\n\t<li>Integration of Flink with the Mahout DSL to support a high level linear algebra abstraction</li>\n\t<li>Integration of H2O with Flink to benefit from H2O's sophisticated machine learning algorithms</li>\n\t<li>Implementation of a parameter server like distributed global state storage facility for Flink. This also includes the extension of Flink to support asynchronous iterations and update messages.</li>\n</ul>\n\n\n<p>Own ideas for a possible contribution on the field of the machine learning library are highly welcome.</p>"}, {"id": "FLINK-1536", "summary": "GSoC project: Graph partitioning operators for Gelly", "link": "https://issues.apache.org/jira/browse/FLINK-1536", "project": "Flink", "votes": "0", "watches": "4", "comments": 11, "labels": ["graph", "gsoc2015", "java"], "description": "<p>Smart graph partitioning can significantly improve the performance and scalability of graph analysis applications. Depending on the computation pattern, a graph partitioning algorithm divides the graph into (maybe overlapping) subgraphs, optimizing some objective. For example, if communication is performed across graph edges, one might want to minimize the edges that cross from one partition to another.</p>\n\n<p>The problem of graph partitioning is a well studied problem and several algorithms have been proposed in the literature. The goal of this project would be to choose a few existing partitioning techniques and implement the corresponding graph partitioning operators for Gelly.</p>\n\n<p>Some related literature can be found <a href=\"http://www.citeulike.org/user/vasiakalavri/tag/graph-partitioning\" class=\"external-link\" rel=\"nofollow\">here</a>.</p>"}, {"id": "FLINK-1534", "summary": "GSoC project: Distributed pattern matching over Flink streaming", "link": "https://issues.apache.org/jira/browse/FLINK-1534", "project": "Flink", "votes": "0", "watches": "3", "comments": 2, "labels": ["gsoc2015", "java", "scala"], "description": "<p>Pattern matching over streams is an important application. The general structure of a streaming pattern matching is the following:</p>\n\n<p>If A event follows B event then trigger some computation.</p>\n\n<p>The support for this feature is associated with complex event processing systems, however it is also adoptable for distributed setting, however it poses additional challenges.</p>\n\n<p>The Google Summer of Code student volunteering for this project is expected to have general knowledge of distributed systems and Java/Scala coding skills. The project includes research and implementation oriented taks. </p>"}, {"id": "FLINK-1503", "summary": "GSoC project: Batch and Streaming integration through new operators and unified API", "link": "https://issues.apache.org/jira/browse/FLINK-1503", "project": "Flink", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015", "java", "scala"], "description": "<p>Currently the Flink batch and streaming API-s (java and scala) and runtimes work independently from each other without operators to allow interactions between DataStreams and DataSets in a fault-tolerant manner.</p>\n\n<p>The goal is to modify the execution environments and the runtime layer to allow these interactions.</p>\n\n<p>Possible runtime changes to add:<br/>\n-Interaction through intermediate files<br/>\n-Interaction by connection the execution graphs </p>\n\n<p>Possible new operators implement:<br/>\n-Converting a dataset to a datastream (either by directly streaming in the results or periodically executing the dataset transformations)<br/>\n-Hash joining a datastream with a dataset by key<br/>\n-Other binary operators with streams and sets</p>\n\n<p>The implementations should work with the fault tolerance mechanism provided by then (exaclty-once or at-least-once guarantees).</p>"}, {"id": "DERBY-6791", "summary": "Google Summer of Code 2015: Derby bug fixing", "link": "https://issues.apache.org/jira/browse/DERBY-6791", "project": "Derby", "votes": "0", "watches": "4", "comments": 13, "labels": ["java", "database", "gsoc2015", "jdbc"], "description": "<p>For the 2015 Google Summer of Code, I am offering to mentor a<br/>\nstudent for general bug fixing of the Derby database.</p>\n\n<p>The Derby JIRA has collected the community's knowledge about<br/>\nknown bugs in Derby, and there are plenty of bugs for us to work on.</p>\n\n<p>If you take on this project, with assistance from me, you'll:</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>select Derby issues from the Derby issue to fix</li>\n\t<li>reproduce those problems by writing and running tests</li>\n\t<li>develop patches to address the problems</li>\n\t<li>work with the community to get the patches reviewed</li>\n\t<li>have your reviewed and accepted contributions committed to the next Derby release.</li>\n</ul>"}, {"id": "COUCHDB-2633", "summary": "Write a new replicator addon", "link": "https://issues.apache.org/jira/browse/COUCHDB-2633", "project": "CouchDB", "votes": "0", "watches": "2", "comments": 0, "labels": ["couchdb", "javascript", "gsoc2015"], "description": "<p>The replicator addon in CouchDB needs some love. The idea is to create a complete new replicator addon for replications between CouchDB servers with a lot new features:</p>\n\n<ul class=\"alternate\" type=\"square\">\n\t<li>Choosing multiple databases (<a href=\"https://issues.apache.org/jira/browse/COUCHDB-1848\" class=\"external-link\" rel=\"nofollow\">https://issues.apache.org/jira/browse/COUCHDB-1848</a>)</li>\n\t<li>persistent replication (<a href=\"https://issues.apache.org/jira/browse/COUCHDB-2145\" class=\"external-link\" rel=\"nofollow\">https://issues.apache.org/jira/browse/COUCHDB-2145</a>)</li>\n\t<li>specifying filters, query parameters and additional options (<a href=\"https://issues.apache.org/jira/browse/COUCHDB-2146\" class=\"external-link\" rel=\"nofollow\">https://issues.apache.org/jira/browse/COUCHDB-2146</a>)</li>\n</ul>\n\n\n\n<p>The project is using React.js for the Admin-Interface. You will use JavaScript, CSS, HTML and the HTTP API of CouchDB.</p>"}, {"id": "COUCHDB-2605", "summary": "Visualize the CouchDB Cluster", "link": "https://issues.apache.org/jira/browse/COUCHDB-2605", "project": "CouchDB", "votes": "0", "watches": "3", "comments": 0, "labels": ["couchdb", "gsoc2015", "javascript"], "description": "<p>Show for each database on which nodes in the cluster the data is stored - and warn if the disk space runs out on these nodes.</p>\n\n<p>The project is using React.js for the Admin-Interface. You will use JavaScript, CSS, HTML and the HTTP API of CouchDB to visualize the cluster. </p>"}, {"id": "COUCHDB-2214", "summary": "Dashboard as main page", "link": "https://issues.apache.org/jira/browse/COUCHDB-2214", "project": "CouchDB", "votes": "0", "watches": "3", "comments": 4, "labels": ["couchdb", "gsoc2015", "javascript"], "description": "<p>Currently, the main page of Fauxton is list of all databases on server. While this representation is traditional and legacy from Futon, it would be awesome to see there some dashboard of summary information.</p>\n\n<p>For instance, you're logged as admin. What Fauxton may show you on there?</p>\n\n<ul class=\"alternate\" type=\"square\">\n\t<li>Recently visited databases (<a href=\"https://issues.apache.org/jira/browse/COUCHDB-2112\" title=\"Show list of recently visited databases\" class=\"issue-link\" data-issue-key=\"COUCHDB-2112\">COUCHDB-2112</a>)</li>\n\t<li>Active replications (+ active_tasks)</li>\n\t<li>Some valuable stats like 500 / 401 HTTP errors</li>\n\t<li>Recent log messages</li>\n\t<li>Quick links for users / replicator databases, create new user/admin</li>\n\t<li>Some field for alert message like: Fix the admin party. Warning! You have the admin party and your CouchDB is accessible not only from localhost. And some other recommendations (like raise max_open_databases in case if you have a lot of dbs and stats tells that you're getting closer to this limit), tip of the day etc.</li>\n</ul>\n\n\n<p>As for regular user there could be showed:</p>\n\n<ul class=\"alternate\" type=\"square\">\n\t<li>Recently visited databases</li>\n\t<li>Link to his profile (doc in _users db)<br/>\n(whatelse may be useful for regular users?)</li>\n</ul>\n\n\n<p>In other words, it would be awesome to have nicer welcome page with useful information as summary from other sources.</p>"}, {"id": "COUCHDB-2163", "summary": "Visualize document revision tree and navigate between these revisions", "link": "https://issues.apache.org/jira/browse/COUCHDB-2163", "project": "CouchDB", "votes": "0", "watches": "2", "comments": 4, "labels": ["couchdb", "gsoc2015", "javascript"], "description": "<p>Futon allows just to navigate between revisions on the same branch.<br/>\nIt would be awesome if Fauxton will visualize revision tree like <a href=\"https://github.com/neojski/visualizeRevTree\" class=\"external-link\" rel=\"nofollow\">visualizeRevTree</a> does and allows to jump to the specific revision in single click.</p>\n\n<p>Currently Fauxton only shows the \"latest\" document's revision with no \"history\" navigation.</p>"}, {"id": "COUCHDB-1787", "summary": "Automate release process documentation", "link": "https://issues.apache.org/jira/browse/COUCHDB-1787", "project": "CouchDB", "votes": "1", "watches": "4", "comments": 6, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>The release process today contains a large number of manual transformation steps.</p>\n\n<p>Fixing this will make the release process significantly easier for release managers, as well as less error-prone.</p>\n\n<p>Ideally the output formats (NEWS, CHANGES in source tree, and HTML snippets for <a href=\"http://couchdb.org/\" class=\"external-link\" rel=\"nofollow\">http://couchdb.org/</a> website and <a href=\"http://blogs.apache.org/couchdb\" class=\"external-link\" rel=\"nofollow\">http://blogs.apache.org/couchdb</a> ) can be auto-generated from either the .rst files in share/doc/src using sphinx's .versionaddded/changed tags, or potentially from commit messages if this is appropriate.</p>\n\n<p>CouchDB documentation is generated today from restructured text using python code, and rolled into the release documentation during `make distcheck`.</p>"}, {"id": "COUCHDB-1754", "summary": "Implement ubjson support", "link": "https://issues.apache.org/jira/browse/COUCHDB-1754", "project": "CouchDB", "votes": "2", "watches": "4", "comments": 10, "labels": ["gsoc2013,", "gsoc2015", "http"], "description": "<p>Just remembered this thing. For a GSOC project it might prove useful as a \"what would it take to support alternative data formats that are representable as JSON\" structures.</p>\n\n<p><a href=\"http://ubjson.org/\" class=\"external-link\" rel=\"nofollow\">http://ubjson.org/</a></p>"}, {"id": "COUCHDB-1743", "summary": "Make the view server & protocol faster", "link": "https://issues.apache.org/jira/browse/COUCHDB-1743", "project": "CouchDB", "votes": "2", "watches": "6", "comments": 16, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>View server protocol enhancements/refactoring - unix sockets, pipelining, different wire format etc. Faster!!</p>"}, {"id": "CONNECTORS-1162", "summary": "Apache Kafka Output Connector", "link": "https://issues.apache.org/jira/browse/CONNECTORS-1162", "project": "ManifoldCF", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc", "gsoc2015"], "description": "<p>Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design. A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients.</p>\n\n<p>Apache Kafka is being used for a number of uses cases. One of them is to use Kafka as a feeding system for streaming BigData processes, both in Apache Spark or Hadoop environment. A Kafka output connector could be used for streaming or dispatching crawled documents or metadata and put them in a BigData processing pipeline</p>"}, {"id": "CONNECTORS-1161", "summary": "Atlassian Confluence Wiki Repository and Authority Connector", "link": "https://issues.apache.org/jira/browse/CONNECTORS-1161", "project": "ManifoldCF", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc", "gsoc2015"], "description": "<p>Confluence is a team collaboration software developed and maintained by Atlassian. Also frequently referred as an Enterprise wiki, Confluence has positioned as one of the most used technologies for sharing knowledge, host documentation and manage content within enterprise.</p>\n\n<p>Confluence can't be crawled using the standard wikis API.Otherwise, Confluence expose it owns API that can consulted here:</p>\n\n<p><a href=\"https://docs.atlassian.com/confluence/REST/latest/\" class=\"external-link\" rel=\"nofollow\">https://docs.atlassian.com/confluence/REST/latest/</a></p>\n\n<p>About permissions, although Confluence can be configured for using external systems like LDAP, it also provides an API for checking users privileges on content:</p>\n\n<p><a href=\"https://developer.atlassian.com/confdev/development-resources/confluence-architecture/confluence-internals/confluence-permissions-architecture\" class=\"external-link\" rel=\"nofollow\">https://developer.atlassian.com/confdev/development-resources/confluence-architecture/confluence-internals/confluence-permissions-architecture</a></p>\n\n<p>More Info: <a href=\"https://www.google.es/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=confluence+permissions+REST+api\" class=\"external-link\" rel=\"nofollow\">https://www.google.es/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=confluence+permissions+REST+api</a></p>"}, {"id": "CONNECTORS-1117", "summary": "Create a livelink connector using the LiveLink REST API", "link": "https://issues.apache.org/jira/browse/CONNECTORS-1117", "project": "ManifoldCF", "votes": "1", "watches": "3", "comments": 3, "labels": ["gsoc2015"], "description": "<p>LAPI is deprecated, so develop a connector that doesn't use it.</p>\n\n<p>Here's API documentation:</p>\n\n<p><a href=\"https://developer.opentext.com/awd/forums/questions/1130441#r1130448\" class=\"external-link\" rel=\"nofollow\">https://developer.opentext.com/awd/forums/questions/1130441#r1130448</a></p>"}, {"id": "CONNECTORS-1058", "summary": "Implement worker thread interruptibility in Alfresco Webscript connector", "link": "https://issues.apache.org/jira/browse/CONNECTORS-1058", "project": "ManifoldCF", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015"], "description": "<p>Background threads need to be added to the connector so it is interruptible.</p>"}, {"id": "COMDEV-125", "summary": "Zeppelin GSoC Project: add R interpreter", "link": "https://issues.apache.org/jira/browse/COMDEV-125", "project": "Community Development", "votes": "0", "watches": "1", "comments": 0, "labels": ["gsoc2015", "java"], "description": "<p>Zeppelin have ability to run remote interpreters though RPS and already has scala and python interpreter implementations with support of Spark and pySpark.</p>\n\n<p>SparkR is going to be part of the Spark project as soon as 1.4 AKA <a href=\"https://issues.apache.org/jira/browse/SPARK-5654\" class=\"external-link\" rel=\"nofollow\">SPARK-5654</a>  and it is a good time to add R interpreter to Zeppelin to support more of the data analytics workflow</p>\n\n<p>The goal of this Google Summer of Code project is to develop an R interpreter witch should be able to run\\integrate with SparkR.</p>\n\n<p>Stunents are welcome to reach out Zeppelin community via its developers mailing list to discuss the details, see <a href=\"http://zeppelin.incubator.apache.org/community.html\" class=\"external-link\" rel=\"nofollow\">http://zeppelin.incubator.apache.org/community.html</a></p>"}, {"id": "COMDEV-123", "summary": "Taverna: Android app Taverna Mobile", "link": "https://issues.apache.org/jira/browse/COMDEV-123", "project": "Community Development", "votes": "0", "watches": "4", "comments": 6, "labels": ["android", "gsoc", "gsoc2015", "java", "mentor", "mobile"], "description": "<p>Project: Apache Taverna (incubating) <a href=\"http://taverna.incubator.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/</a><br/>\nMentor: Ian Dunlop &lt;ianwdunlop@apache.org&gt;, Stian Soiland-Reyes &lt;stain@apache.org&gt;</p>\n\n<p>Taverna Mobile: A year or so ago in conjunction with a University of<br/>\nManchester undergraduate we developed an Android app to run Taverna<br/>\nworkflows available on myExperiment (<a href=\"http://www.myexperiment.org\" class=\"external-link\" rel=\"nofollow\">http://www.myexperiment.org</a>)<br/>\nusing Taverna Server. </p>\n\n<p>It is available here <a href=\"https://github.com/myGrid/taverna-mobile\" class=\"external-link\" rel=\"nofollow\">https://github.com/myGrid/taverna-mobile</a>. Later on we updated it to use Ahe android IDE from google/idea and the gradle build system as well<br/>\nas fixing some of the dependencies, see<br/>\n<a href=\"https://github.com/ianwdunlop/tavernamobile\" class=\"external-link\" rel=\"nofollow\">https://github.com/ianwdunlop/tavernamobile</a>.</p>\n\n<p>The code relies on some APIs from google and apache which have moved<br/>\non since then eg the https libraries which needs updated.<br/>\nThe project would be to:<br/>\n1) fix the ssl/https parts and use the latest version of taverna server<br/>\n2) update the code to remove some legacy dependencies and use the<br/>\nlatest APIs<br/>\n3) improve the UI and responsiveness</p>\n\n<p>1) &amp; 2) are the most important bits with 3) being a bit extra<br/>\ndepending on how the first bits progress.</p>\n\n<p>The project would interest students with a good grounding in Java<br/>\nlooking to get into Android mobile development.</p>\n\n<p>You would be part of he Apache Taverna community<br/>\n<a href=\"http://taverna.incubator.apache.org/community/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/community/</a><br/>\nwhich would be giving advice and feedback on your<br/>\napp.</p>"}, {"id": "COMDEV-122", "summary": "Taverna: Open Geospatial Consortium Web Processing Service support", "link": "https://issues.apache.org/jira/browse/COMDEV-122", "project": "Community Development", "votes": "0", "watches": "2", "comments": 2, "labels": ["gsoc", "gsoc2015", "java", "mentor"], "description": "<p>Project: Apache Taverna (incubating) <a href=\"http://taverna.incubator.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/</a><br/>\nMentor: Alan Robert Williams &lt;alaninmcr@apache.org&gt;</p>\n\n<p>Apache Taverna, <a href=\"http://taverna.incubator.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/</a>,  \u201cis an open source and domain-independent Workflow Management System \u2013 a suite of tools used to design and execute scientific workflows and aid in silico experimentation\u201d. The workflows consist of instances of services joined by connections. There are different types of services, such as making a REST call, running an R script or requesting information from a user.</p>\n\n<p>\u201cThe Open Geospatial Consortium (OGC) is an international industry consortium of 511 companies, government agencies and universities participating in a consensus process to develop publicly available interface standards. OGC\u00ae Standards support interoperable solutions that \"geo-enable\" the Web, wireless and location-based services and mainstream IT. The standards empower technology developers to make complex spatial information and services accessible and useful with all kinds of applications.\u201d - <a href=\"http://www.opengeospatial.org/ogc\" class=\"external-link\" rel=\"nofollow\">http://www.opengeospatial.org/ogc</a> </p>\n\n<p>\u201cThe OGC\u2019s Web Processing Standard Service Interface Standard provides rules for standardizing how inputs and outputs (requests and responses) for geospatial processing services, such as polygon overlay. The standard also defines how a client can request the execution of a process, and how the output from the process is handled. It defines an interface that facilitates the publishing of geospatial processes and clients\u2019 discovery of and binding to those processes. The data required by the WPS can be delivered across a network or they can be available at the server.\u201d - <a href=\"http://www.opengeospatial.org/standards/wps\" class=\"external-link\" rel=\"nofollow\">http://www.opengeospatial.org/standards/wps</a> </p>\n\n<p>There have been several requests from users, including two in 2015, for Taverna to be able to include the calling of WPS services within workflows.</p>\n\n<p>The WPS services can be discovered and invoked by either WSDL operations or REST calls to the WPS Server. However, the operations of a server do not take account of the processes that are available on the server. Instead, they allow you to ask what processes are known to the server, and to execute a process on the server. Exactly the same operations are used to call different processes. You have to specify the process name as a parameter to the operation.</p>\n\n<p>These generic operations are not very useful when you want to call specific processes in a workflow. Instead, you want an operation to call a specific process. So each process known to the server would have its own operation(s).</p>\n\n<p>One server-side solution, PyWPS has been developed <a href=\"http://pywps.wald.intevation.org/\" class=\"external-link\" rel=\"nofollow\">http://pywps.wald.intevation.org/</a> . PyWPS makes a WSDL that calls the specific operations. However, this involves making changes to the WPS server, to which the workflow designer may not have access. It is also only available for installation on certain types of WPS server.</p>\n\n<p>Some initial work has been made to create Taverna-side solutions, so that Taverna would be able to discover the WPS services and to invoke them, <a href=\"https://github.com/zzpwelkin/taverna-wps-plugin.git\" class=\"external-link\" rel=\"nofollow\">https://github.com/zzpwelkin/taverna-wps-plugin.git</a> and <a href=\"https://github.com/alaninmcr/wps-taverna.git\" class=\"external-link\" rel=\"nofollow\">https://github.com/alaninmcr/wps-taverna.git</a> . Both of these have limitations and have not been extensively tested. They are similar in their use of the 52\u00b0North WPS library <a href=\"http://52north.org/communities/geoprocessing/wps\" class=\"external-link\" rel=\"nofollow\">http://52north.org/communities/geoprocessing/wps</a> . They also work with Taverna 2.5 rather than the currently being developed Taverna 3.</p>\n\n<p>This proposed GSOC project is to, in collaboration with dev@taverna and the mentor</p>\n\n<ol>\n\t<li>Improve/complete the existing Taverna 2.5 WPS prototype support\n\t<ol>\n\t\t<li>Discovery of WPS services from a user-specified WPS server</li>\n\t\t<li>Configuration of WPS services</li>\n\t\t<li>Execution of WPS services</li>\n\t</ol>\n\t</li>\n\t<li>Test WPS support using pre-selected WPS servers and services</li>\n\t<li>Migrate the WPS support to the Taverna 3 codebase</li>\n\t<li>Document the Taverna 3 WPS service support</li>\n</ol>\n\n\n<p>There are other related OGC standards such as the Web Map Service (WMS) and Sensor Observation Service (SOS). The project can be extended to include the discovery and execution of such services.</p>\n\n<p>Your WPS service would be added to Apache Taverna, so you would be a part of the Apache Taverna developer community <a href=\"http://taverna.incubator.apache.org/community/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/community/</a> which will be able to give feedback, testing and guidance for this GSOC project and beyond.</p>"}, {"id": "COMDEV-121", "summary": "Taverna: Databundle viewer for web", "link": "https://issues.apache.org/jira/browse/COMDEV-121", "project": "Community Development", "votes": "0", "watches": "3", "comments": 3, "labels": ["gsoc", "gsoc2015", "html", "mentor", "rest", "web"], "description": "<p>Project: Apache Taverna (incubating) <a href=\"http://taverna.incubator.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/</a><br/>\nMentor: Robert Haines&lt;hainesr@apache.org&gt;, Stian Soiland-Reyes &lt;stain@apache.org&gt;, Ian Dunlop &lt;ianwdunlop@apache.org&gt;</p>\n\n<p>Taverna Databundle is a file format returned by the Taverna Server containing the results of a Taverna workflow run and its intermediate values and provenance metadata.</p>\n\n<p>The databundle is a structured ZIP file with metadata in JSON-LD.<br/>\n<a href=\"https://github.com/apache/incubator-taverna-language/tree/master/taverna-databundle\" class=\"external-link\" rel=\"nofollow\">https://github.com/apache/incubator-taverna-language/tree/master/taverna-databundle</a></p>\n\n<p>This GSOC project proposes to create a web-based presentation (but not edit) of a workflow run, e.g. the following user story:</p>\n\n<p>1) User uploads the databundle file for a workflow run (or provides the URI to one existing elsewhere) into the presentation system<br/>\n2) The presentation system shows basic information about the workflow run<br/>\n3) User navigates to inputs/outputs<br/>\n4) The presentation system presents a list of inputs or outputs - embedding images etc. where possible, download links for large files, inline for small text, etc<br/>\n5) User downloads a selected data file</p>\n\n<p>Metadata provided can be presented in many different ways, for instance:</p>\n\n<p>a) Which workflow was executed?<br/>\nb) Which steps were executed?<br/>\nc) What step produced this value?<br/>\nd) What values were produced by this step?<br/>\ne) What steps used this value?</p>\n\n<p>The student is free to choose programming language and web framework, as long as a free runtime is available (e.g. Ruby on Rails, Java CXF, Node.js, Bootstrap, Ember).</p>\n\n<p>Your web viewer should aim to be added to the Apache Taverna as a release, so you would be a part of the Apache Taverna developer community <a href=\"http://taverna.incubator.apache.org/community/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/community/</a><br/>\nwhich will be able to give feedback, testing and guidance for this GSOC project and beyond.</p>\n\n<p>We can provide virtual servers for hosting the viewer prototypes.</p>"}, {"id": "COMDEV-120", "summary": "Taverna: Language command line tool", "link": "https://issues.apache.org/jira/browse/COMDEV-120", "project": "Community Development", "votes": "1", "watches": "2", "comments": 4, "labels": ["gsoc", "gsoc2015", "java", "mentor"], "description": "<p>Project: Apache Taverna (incubating) <a href=\"http://taverna.incubator.apache.org/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/</a><br/>\nMentor: Stian Soiland-Reyes &lt;stain@apache.org&gt;</p>\n\n<p>Apache Taverna Language - <a href=\"https://github.com/apache/incubator-taverna-language/\" class=\"external-link\" rel=\"nofollow\">https://github.com/apache/incubator-taverna-language/</a> - is a set of Java APIs for managing and converting Taverna workflow definitions.</p>\n\n<p>Several simple demonstrator/example prototypes for a command line tool over these has been done, e.g. <br/>\n<a href=\"https://github.com/apache/incubator-taverna-language/tree/master/taverna-scufl2-examples\" class=\"external-link\" rel=\"nofollow\">https://github.com/apache/incubator-taverna-language/tree/master/taverna-scufl2-examples</a><br/>\n<a href=\"https://github.com/apache/incubator-taverna-language/tree/master/taverna-scufl2-wfdesc\" class=\"external-link\" rel=\"nofollow\">https://github.com/apache/incubator-taverna-language/tree/master/taverna-scufl2-wfdesc</a><br/>\n<a href=\"https://github.com/stain/ro-combine-archive\" class=\"external-link\" rel=\"nofollow\">https://github.com/stain/ro-combine-archive</a></p>\n\n<p>but what is needed is a single command line tool that is production ready, and ideally available as an executable standalone jar (As in the wfdesc example).</p>\n\n<p>This proposed GSOC project is to formalize the command line interface for Taverna Language so that it can achieve some of the below:</p>\n\n<p>a) In collaboration with dev@taverna and the mentor, define command line options for workflow conversions and inspection<br/>\nb) Find and use an appropriate command line parser library<br/>\nc) --help<br/>\nd) Basic error handling (e.g. file not found)<br/>\ne) Batch processing - e.g. recursive conversions<br/>\nf) Arguments for choosing input and output formats<br/>\ng) Unit and Integration Tests for the above (ideally through Maven)</p>\n\n<p>The project can be extended by exposing other parts of the API that don't currently have a command line equivalent, e.g.</p>\n\n<p>h) Workflow validation<br/>\ni) Workflow statistics (number of processors, links, etc)</p>\n\n<p>Your command line tool would be added to the Apache Taverna Language release, so you would be a part of the Apache Taverna developer community <a href=\"http://taverna.incubator.apache.org/community/\" class=\"external-link\" rel=\"nofollow\">http://taverna.incubator.apache.org/community/</a><br/>\nwhich will be able to give feedback, testing and guidance for this GSOC project and beyond.</p>"}, {"id": "COMDEV-119", "summary": "Zeppelin GSoC Project: add more D3 visualization", "link": "https://issues.apache.org/jira/browse/COMDEV-119", "project": "Community Development", "votes": "0", "watches": "5", "comments": 3, "labels": ["gsoc2015", "javascript", "visualization"], "description": "<p>Zeppelin frontend web application already have rich visualization library based on D3.</p>\n\n<p>The goal of this Google Summer of Code project is to add more visualizations from the wide range of existing ones in D3.</p>\n\n<p>This might be a good opportunity for a students, interested in data visualization to learn industry standard tools.</p>\n\n<p>Stunents are welcome to reach out Zeppelin community via its developers mailing list to discuss the dtails, see <a href=\"http://zeppelin.incubator.apache.org/community.html\" class=\"external-link\" rel=\"nofollow\">http://zeppelin.incubator.apache.org/community.html</a></p>"}, {"id": "COMDEV-118", "summary": "Zeppelin GSoC Project: create a ML\\Deeplearning tutorial notebook", "link": "https://issues.apache.org/jira/browse/COMDEV-118", "project": "Community Development", "votes": "0", "watches": "4", "comments": 5, "labels": ["deeplearning", "gsoc2015", "machine_learning", "scala", "spark"], "description": "<p>Zeppelin as generic notebook interpreter system for cluster computing is capable of connecting different distributed systems. The main focus right now is Apache Spark which itself have extensive machine learning facilities.</p>\n\n<p>There are other systems such as H2O, PredictIO, Parameterserver or Singa implementing advanced techniques based on neural networks, commonly referred as Deeplearning.</p>\n\n<p>The goal of this Google Summer of Code project is to develop a tutorial Notebook with a sample data and extend Zeppelin server to be able to show advantages of interactive machine learning.</p>\n\n<p>Stunents are welcome to reach out Zeppelin community via its developers mailing list to discuss the details, see <a href=\"http://zeppelin.incubator.apache.org/community.html\" class=\"external-link\" rel=\"nofollow\">http://zeppelin.incubator.apache.org/community.html</a></p>"}, {"id": "COMDEV-117", "summary": "Zeppelin GSoC Project: expand coverage of unit and integration tests for Angular.js webapp", "link": "https://issues.apache.org/jira/browse/COMDEV-117", "project": "Community Development", "votes": "0", "watches": "2", "comments": 4, "labels": ["angularjs", "gsoc2015", "javascript"], "description": "<p>Zeppelin frontend is an Angular.js web-application with a quite simple model. <br/>\nIt is Typescript\\Javascript based and although the build infrastructure is set up (Karma\\Jasmine) it has very low test coverage right now.</p>\n\n<p>The goal of this Google Summer of Code project is to create test scenarios for controllers\\directives and boos a test coverage. </p>\n\n<p>This is a good way to learn Angular and software engineering best practices for modern web applications.</p>\n\n<p>Stunents are welcome to reach out Zeppelin community via its developers mailing list to discuss the details, see <a href=\"http://zeppelin.incubator.apache.org/community.html\" class=\"external-link\" rel=\"nofollow\">http://zeppelin.incubator.apache.org/community.html</a></p>"}, {"id": "COMDEV-116", "summary": "Implement JSR 196 (JASPIC) for Apache Tomcat", "link": "https://issues.apache.org/jira/browse/COMDEV-116", "project": "Community Development", "votes": "0", "watches": "1", "comments": 2, "labels": ["gsoc2015"], "description": "<p>This project is to implement the Java Authentication Service Provider Interface for Containers (JASPIC) - JSR 196 - for Apache Tomcat and replace the current authentication modules with JASPIC equivalents.</p>\n\n<p>The intention is to implement this for Apache Tomcat 9. It is possible but unlikely that it would also be back-ported to earlier versions.</p>\n\n<p>The development language will be Java.</p>\n\n<p>References:<br/>\nJASPIC Specification:<br/>\n<a href=\"https://www.jcp.org/en/jsr/detail?id=196\" class=\"external-link\" rel=\"nofollow\">https://www.jcp.org/en/jsr/detail?id=196</a></p>\n\n<p>Contributing to Apache Tomcat:<br/>\n<a href=\"http://tomcat.apache.org/getinvolved.html\" class=\"external-link\" rel=\"nofollow\">http://tomcat.apache.org/getinvolved.html</a><br/>\n<a href=\"http://tomcat.apache.org/svn.html\" class=\"external-link\" rel=\"nofollow\">http://tomcat.apache.org/svn.html</a></p>\n\n<p>Previous discussions:<br/>\n<a href=\"http://tomcat.markmail.org/search/JASPIC\" class=\"external-link\" rel=\"nofollow\">http://tomcat.markmail.org/search/JASPIC</a><br/>\n<a href=\"https://java.net/projects/servlet-spec/lists/users/archive/2014-10/message/4\" class=\"external-link\" rel=\"nofollow\">https://java.net/projects/servlet-spec/lists/users/archive/2014-10/message/4</a></p>"}, {"id": "CLOUDSTACK-8310", "summary": "commit to commit db upgrades and db version control", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8310", "project": "CloudStack", "votes": "0", "watches": "2", "comments": 4, "labels": ["gsoc2015"], "description": "<p>Cloudstack currently uses a homegrown tool to do the database upgrades. <br/>\nThe challenges with the current one being, it can only do version to version but not commit to commit upgrades.<br/>\nDue to this, when different people work on the same branch and have db changes, environment is broken untill you do fresh db deploy.</p>\n\n<p>To achieve this, we can use existing and well tested tools like liquibase, flywaydb etc. or improve on the existing one. </p>\n\n<p>Related discussions on dev lists<br/>\n<a href=\"http://markmail.org/thread/aicijeu6g5mzx4sc\" class=\"external-link\" rel=\"nofollow\">http://markmail.org/thread/aicijeu6g5mzx4sc</a><br/>\n<a href=\"http://markmail.org/thread/r7wv36o356nolq7f\" class=\"external-link\" rel=\"nofollow\">http://markmail.org/thread/r7wv36o356nolq7f</a></p>"}, {"id": "CLOUDSTACK-8275", "summary": "Deploying vApp(Stack of services/instances) with user defined template from CloudStack", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8275", "project": "CloudStack", "votes": "0", "watches": "2", "comments": 5, "labels": ["gsoc2015"], "description": "<p>This idea will help the cloud admin/user to deploy the Stack of instance which can act as his service solution. Use Case: Suppose cloud admin/user wants to deploy a web service portal and he has presentation template, logic tier template and database template. In the present situation, the user had to provision Vm from each template and then do the configuration so that they can logically act as one service stack. He has to create a network (deployVR) &amp; then he has to deploy multiple instances from each template and then later configure them to act as one solution.</p>"}, {"id": "CLOUDSTACK-8266", "summary": "Automatic workload Management in the clusters Managed by CloudStack for efficient host Management.", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8266", "project": "CloudStack", "votes": "1", "watches": "4", "comments": 5, "labels": ["gsoc2015"], "description": "<p>CloudStack manages hypervisor hosts as clusters. It will deploy the vm's to ensure proper hosts capacity are used max. <br/>\nDue to some operations by user on vm's like stop/migrate/destroy hosts usage in the clusters will be in inefficient state. <br/>\nTo solve this issue we can have a new Manager who will regularly checks host capacity and implement Server Consolidtion by taking the actions: </p>\n\n<p>1. Live migrate the vm's if possible along with storage. Migrate them such that hosts are fully utilized. </p>\n\n<p>2. After redistributing the vm's Shutdown the hosts which are empty. </p>\n\n<p>3. When load is getting increased,power-on the hosts remotely by WOL(Wake On Lan) mechanisam.</p>\n\n\n<p>This idea is discussed and presented in Collab Conference 2014.</p>\n\n<p>Here is the youtube link</p>\n\n<p><a href=\"https://www.youtube.com/watch?v=hJKdZcSpGNQ\" class=\"external-link\" rel=\"nofollow\">https://www.youtube.com/watch?v=hJKdZcSpGNQ</a></p>"}, {"id": "CLOUDSTACK-8249", "summary": "Dockerize CloudStack deployment", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8249", "project": "CloudStack", "votes": "0", "watches": "3", "comments": 0, "labels": ["gsoc2015"], "description": "<p>CloudStack like any IaaS can be daunting to install.</p>\n\n<p>I propose to Dockerize the entire deployment process. There is already a few docker images for the simulator. The project will consist of two steps:</p>\n\n<p>1-Dockerize the mgt server, including separate container for the MySQl DB and the usage server.</p>\n\n<p>2-Dockerize the agent (assuming KVM ). Potentially even run KVM with Docker to contain everything.</p>\n\n<p>3-Setup a self discovery mechanism with tools like register, confd, or etcd.</p>\n\n<p>4-Automatically configure CloudStack based on self-registered services.</p>"}, {"id": "CLOUDSTACK-8227", "summary": "Creating VDI support tool for cloudstack", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8227", "project": "CloudStack", "votes": "0", "watches": "3", "comments": 8, "labels": ["cloud", "gsoc2015", "java"], "description": "<p>I wish to create a VDI support tool for cloudstack so that DesktopAsAService can be provided using Cloudstack. I wish to use Guacamole Web Service (<a href=\"http://guac-dev.org/\" class=\"external-link\" rel=\"nofollow\">http://guac-dev.org/</a>) for the purpose which is based on VNC to do the same</p>"}, {"id": "CLOUDSTACK-8223", "summary": "generate ui widgets for api sets", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8223", "project": "CloudStack", "votes": "0", "watches": "2", "comments": 2, "labels": ["gsoc2015"], "description": "<p>based on update, create and list api calls widgets that can be used in custom user intefaces can be created. A good framework must be choosen and generation code written. Most likely a v2 api must be created to deal with the inconsistencies in the present api.</p>"}, {"id": "CLOUDSTACK-8208", "summary": "Improve CloudStack Integration Testing and Write tool for automating it", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8208", "project": "CloudStack", "votes": "0", "watches": "2", "comments": 3, "labels": ["cloud", "golang", "gsoc2015", "java", "python"], "description": "<p>The integration tests that CloudStack has are hard to run on real hardware due to strict/hardcoded configuration. The task of this project are:</p>\n\n<ul class=\"alternate\" type=\"square\">\n\t<li>Figure about minimal system resources needed to run all integration tests (RAM, CPU, no. of VMs).</li>\n\t<li>Fix integration tests for they can run on a developer's laptop or mini PCs such as NUC with real hypervisors (and not just simulator) - Xen or KVM. All hypervisors run in nested virtualized environment - for example Xen on VirtualBox, KVM on VMWare workstation or Fusion, or KVM/Xen on KVM etc.</li>\n\t<li>Create Jenkins job for the same</li>\n\t<li>Write an tool necessary to automate this</li>\n\t<li>Create ansible based (ideal/template) CloudStack deployment based on Xen or KVM (checkout as an example, github.com/bhaisaab/peppercorn)</li>\n</ul>\n\n\n<p>This will be most important GSoC project and contribution to CloudStack if it delivers the above because right now even if we've the integration tests, they are hard to run by developers.</p>"}, {"id": "CLOUDSTACK-8207", "summary": "Set of small CloudStack tools", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8207", "project": "CloudStack", "votes": "0", "watches": "2", "comments": 2, "labels": ["cloud", "golang", "gsoc2015", "java", "python"], "description": "<p>Develop tools for CloudStack:</p>\n\n<ul class=\"alternate\" type=\"square\">\n\t<li>CloudMonkey enhancements: Refactor CloudMonkey so anyone can write color, output, network and other plugins for CloudMonkey. Add library support to merge cloudmonkey and marvin, so cloudmonkey can also be used to write Python scripts. Add Bash helper methods so it can be used to write bash based automation scripts. Write Ansible module for CloudStack using CloudMonkey or Marvin.</li>\n\t<li>Write a new Database tool using DatabaseUpgraderClass in Python or other language for CloudStack to do db dump, upgrades, backups etc</li>\n\t<li>Write a new tool to do smoke testing (on Jenkins or TravisCI) or integration testing</li>\n\t<li>Write a tool to import/save a CloudStack deployment and export a CloudStack cloud using a saved configuration</li>\n</ul>"}, {"id": "CLOUDSTACK-8206", "summary": "Support Bhyve as a hypervisor in CloudStack", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8206", "project": "CloudStack", "votes": "2", "watches": "3", "comments": 4, "labels": ["cloud", "gsoc", "gsoc2015", "java"], "description": "<p>Support Bhyve (from FreeBSD community) as a hypervisor in CloudStack. This would require using libvirt, and seeing what is possible wrt basic/advance zone and isolated/shared networking.</p>\n\n<p>Suggested Mentor: Rohit Yadav</p>"}, {"id": "CLOUDSTACK-8205", "summary": "Support Docker as a hypervisor in CloudStack", "link": "https://issues.apache.org/jira/browse/CLOUDSTACK-8205", "project": "CloudStack", "votes": "0", "watches": "3", "comments": 4, "labels": ["cloud", "gsoc2015", "java"], "description": "<p>Support Docker as a hypervisor in CloudStack. See what is possible basic/advance zone and shared/isolated networking.</p>"}, {"id": "AURORA-1164", "summary": "Interactive web-based Aurora CLI tutorial", "link": "https://issues.apache.org/jira/browse/AURORA-1164", "project": "Aurora", "votes": "0", "watches": "2", "comments": 2, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>To make Aurora easier to understand, it would be great to have an interactive online tutorial, preferably one that helps you understand the Aurora CLI and value of Aurora without having to spin up a vagrant box. I've seen what Docker has developed to get folks started (<a href=\"https://www.docker.com/tryit/\" class=\"external-link\" rel=\"nofollow\">https://www.docker.com/tryit/</a>), and it seems like something that would be hugely beneficial to replicate in some way.</p>\n\n<p>Unfortunately, the web-based terminal library that Docker uses is licensed under GPL, but there seem to be a variety of MIT, BSD, and Apachev2 licensed libraries that may help someone who takes this on.</p>\n\n<p>I've created a similar issue for Mesos (<a href=\"https://issues.apache.org/jira/browse/MESOS-2445\" title=\"Interactive web-based Mesos CLI tutorial\" class=\"issue-link\" data-issue-key=\"MESOS-2445\">MESOS-2445</a>) &#8211; if they get to this first, perhaps we can reuse their code.</p>"}, {"id": "ANY23-249", "summary": "Update all W3C and other Standards Compliance within Any23", "link": "https://issues.apache.org/jira/browse/ANY23-249", "project": "Apache Any23", "votes": "0", "watches": "3", "comments": 7, "labels": ["gsoc2015"], "description": "<p><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=michele.mostarda\" class=\"user-hover\" rel=\"michele.mostarda\">Michele Mostarda</a> and <a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lewismc\" class=\"user-hover\" rel=\"lewismc\">Lewis John McGibbney</a> have been <a href=\"http://www.mail-archive.com/dev%40any23.apache.org/msg01517.html\" class=\"external-link\" rel=\"nofollow\">discussing</a> what would work well for an Any23 Google Summer of Code Project for 2015.</p>\n\n<p>It turns out that in order to rebuild confidence with the Any23 standards compliance (in light of new W3c standards which may be emerged or advanced) an in light of new non-W3C emerging standards such as <a href=\"http://microformats.org/wiki/microformats-2\" class=\"external-link\" rel=\"nofollow\">microformats2</a> it would be a very worthwhile effort to have one or more student(s) engage on </p>\n<ul>\n\t<li>initially evaluating all of the existing standards compliance within Any23</li>\n\t<li>uncovering which aspects of the compiled list require attention e.g. updating, overhauling, re-implementation, extension or otherwise</li>\n\t<li>progress on executing the above under supervision of one or more of the assigned mentors</li>\n</ul>"}, {"id": "AIRAVATA-1625", "summary": "[GSoC] Integrate Load Balancers to Airavata Services", "link": "https://issues.apache.org/jira/browse/AIRAVATA-1625", "project": "Airavata", "votes": "0", "watches": "2", "comments": 0, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>Airavata components and API servers are developed as Apache Thrift based Services. Some of the Airavata components such as GFac are inherently load balanced at the architecture level. This project should explore an appropriate load balancers for Airavata Services. Note Airavata is a multi-tenant software and an important criteria should be tentant aware load balancing. Also, all services are run as binary over TCP mode. </p>"}, {"id": "AIRAVATA-1624", "summary": "[GSoC] Securing Airavata API", "link": "https://issues.apache.org/jira/browse/AIRAVATA-1624", "project": "Airavata", "votes": "0", "watches": "4", "comments": 9, "labels": ["gsoc", "gsoc2015", "mentor"], "description": "<p>Apache Airavata uses Thrift based API's for external facing API's and for system internal CPI's. The API's need to be secured adding authentication and authorization capabilities. </p>\n\n<p>The Authentication need to ensure only approved users/clients can communicate. Similarly clients should only interact with valid servers. </p>\n\n<p>Authorization need to be enforced to ensure only users with specific roles can appropriately access specific API's. As an example, administrative roles should be able see all the users experiments where as end users can only see his/her data and not access other information (unless explicitly shared). </p>\n\n<p>Earlier GSoC project focused on this topic has relavent discussion. <br/>\n<a href=\"https://cwiki.apache.org/confluence/display/AIRAVATA/GSoC+2014+-+Add+Security+capabilities+to+Airavata+Thrift+services+and+clients\" class=\"external-link\" rel=\"nofollow\">https://cwiki.apache.org/confluence/display/AIRAVATA/GSoC+2014+-+Add+Security+capabilities+to+Airavata+Thrift+services+and+clients</a></p>"}]}